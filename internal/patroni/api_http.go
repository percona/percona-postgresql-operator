package patroni

import (
	"bytes"
	"context"
	"crypto/tls"
	"crypto/x509"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"slices"
	"strings"
	"time"

	"github.com/go-logr/logr"
	"github.com/percona/percona-postgresql-operator/internal/logging"
	"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/trace"
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type podMetadata struct {
	Name         string
	Namespace    string
	InstanceName string
}

func (m podMetadata) getPatroniServerUrl() string {
	return fmt.Sprintf("https://%s.%s-pods.%s.svc:8008", m.Name, m.Namespace, m.Namespace)
}

// The operator creates a "${instanceName}-certs" secret for every Postgres instance.
// It contains pgbouncer, patroni and postgres certificates generated by the operator's own rootCA.
// We need it to call Patroni Rest API server.
func (m podMetadata) getSecretCertKey() client.ObjectKey {
	return client.ObjectKey{
		Name:      fmt.Sprintf("%s-certs", m.InstanceName),
		Namespace: m.Namespace,
	}
}

// Attempt to extract pod metadata from the pod name. Note that this a
// known pattern: pod names include both the namespace (database name) and
// instance name.
//
// Example:
// - podName: pgdb-609qv5o187x841r2-instance1-h8q2-0
// - instanceName: pgdb-609qv5o187x841r2-instance1-h8q2
// - namespace: pgdb-609qv5o187x841r2
func extractMetadataFromPodName(podName string) (podMetadata, error) {
	// 1. Check number of "-" to make sure name has the correct format
	dashes := strings.Count(podName, "-")
	if dashes != 4 {
		err := fmt.Errorf("Member name has unexpected format: name=%s, dashes=%d", podName, dashes)
		return podMetadata{}, err
	}

	// 2. Remove everything after the last dash and we get instanceName: pgdb-609qv5o187x841r2-instance1-h8q2
	lastDashIndex := strings.LastIndex(podName, "-")
	instanceName := podName[:lastDashIndex]

	// 3. Remove everything after the second dash and we get namespace: pgdb-609qv5o187x841r2
	lastDashIndex = strings.LastIndex(instanceName, "-")
	lastDashIndex = strings.LastIndex(instanceName[:lastDashIndex], "-")
	namespace := instanceName[:lastDashIndex]

	return podMetadata{
		Name:         podName,
		Namespace:    namespace,
		InstanceName: instanceName,
	}, nil
}

// A client to a Patroni server. There is one per pod.
type instanceClient struct {
	baseUrl    string
	httpClient *http.Client
	logger     logr.Logger
}

type patroniEndpoint string

const (
	patroniEndpointSwitchover patroniEndpoint = "switchover"
	patroniEndpointFailover   patroniEndpoint = "failover"
	patroniEndpointRestart    patroniEndpoint = "restart"
	patroniEndpointConfig     patroniEndpoint = "config"
	patroniEndpointCluster    patroniEndpoint = "cluster"
)

func (p instanceClient) do(
	ctx context.Context,
	method string,
	endpoint patroniEndpoint,
	body map[string]any,
) (*http.Response, error) {
	bodyInBytes, err := json.Marshal(body)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal configuration: %w", err)
	}

	req, err := http.NewRequestWithContext(
		ctx,
		method,
		fmt.Sprintf("%s/%s", p.baseUrl, endpoint),
		bytes.NewReader(bodyInBytes),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create HTTP request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Accept", "application/json")
	return p.httpClient.Do(req)
}

// Pattern:
//
//	{
//			"name": "pgdb-609qv5o187x841r2-instance1-h8q2-0",
//			"role": "leader",
//			"state": "running",
//			"api_url": "https://pgdb-609qv5o187x841r2-instance1-h8q2-0.pgdb-609qv5o187x841r2-pods:8008/patroni",
//			"host": "pgdb-609qv5o187x841r2-instance1-h8q2-0.pgdb-609qv5o187x841r2-pods",
//			"port": 5432,
//			"timeline": 8
//	}
//
// InstanceName: pgdb-609qv5o187x841r2-instance1-h8q2
// Namespace: pgdb-609qv5o187x841r2
type clusterMember struct {
	Name     string `json:"name"`
	Role     string `json:"role"`
	State    string `json:"state"`
	Host     string `json:"host"`
	Timeline int64  `json:"timeline"`
}

func (p instanceClient) getMembers(ctx context.Context) ([]clusterMember, error) {
	resp, err := p.do(ctx, http.MethodGet, patroniEndpointCluster, nil)
	if err != nil {
		return []clusterMember{}, fmt.Errorf("failed to get cluster status: %w", err)
	}

	defer resp.Body.Close()

	if resp.StatusCode != 200 {
		body, _ := io.ReadAll(resp.Body)
		return []clusterMember{}, fmt.Errorf(
			"cluster status request failed with status %d: %s",
			resp.StatusCode,
			string(body),
		)
	}

	var clusterStatus struct {
		Members []clusterMember `json:"members"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&clusterStatus); err != nil {
		return []clusterMember{}, fmt.Errorf("failed to parse cluster status response: %w", err)
	}

	return clusterStatus.Members, nil
}

func (p instanceClient) getMembersByRole(ctx context.Context, roles []string) ([]clusterMember, error) {
	members, err := p.getMembers(ctx)
	if err != nil {
		return []clusterMember{}, err
	}

	filtered := []clusterMember{}
	for _, member := range members {
		if slices.Contains(roles, member.Role) {
			filtered = append(filtered, member)
		}
	}

	return filtered, nil
}

// Get current cluster leader. It can also be a "primary", so we test for both.
func (p instanceClient) getLeader(ctx context.Context) (clusterMember, error) {
	members, err := p.getMembersByRole(ctx, []string{"leader", "primary"})
	if err != nil {
		return clusterMember{}, err
	}

	if len(members) == 0 {
		return clusterMember{}, fmt.Errorf("no leader found")
	}

	return members[0], nil
}

func (p instanceClient) putConfig(ctx context.Context, configuration map[string]any) error {
	p.logger.Info("Replacing configuration")

	resp, err := p.do(ctx, http.MethodPut, patroniEndpointConfig, configuration)
	if err != nil {
		err := fmt.Errorf("failed to execute HTTP request: %w", err)
		p.logger.Error(err, "Failed to replace configuration")
		return err
	}

	defer resp.Body.Close()

	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		body, _ := io.ReadAll(resp.Body)
		err := fmt.Errorf("HTTP request failed with status %d: %s", resp.StatusCode, string(body))
		p.logger.Error(err, "Failed to replace configuration")
		return err
	}

	p.logger.Info("Successfully replaced configuration", "status", resp.Status)
	return nil
}

func (p instanceClient) switchover(ctx context.Context, leader, candidate string) (bool, error) {
	p.logger.Info("Requesting switchover")

	if leader == "" {
		// The switchover Rest API *requires* a leader, while in patronictl it's autodetected.
		// Ref.: https://patroni.readthedocs.io/en/latest/patronictl.html#patronictl-switchover
		err := fmt.Errorf("leader is required for switchover")
		p.logger.Error(err, "Switchover failed. Leader is required")
		return false, err
	}

	// NOTE: The REST API uses "leader" field (not "master" or "primary")
	requestBody := map[string]any{
		"leader": leader,
	}

	// Optional. If set, patroni will prefer this candidate for leader.
	// Otherwise, it will figure out the next leader by itself.
	if candidate != "" {
		requestBody["candidate"] = candidate
	}

	resp, err := p.do(ctx, http.MethodPost, patroniEndpointSwitchover, requestBody)
	if err != nil {
		p.logger.Error(err, "Switchover failed")
		return false, fmt.Errorf("failed to execute switchover request: %w", err)
	}

	defer resp.Body.Close()
	body, _ := io.ReadAll(resp.Body)
	bodyStr := string(body)

	p.logger.V(1).Info("switchover response", "status", resp.StatusCode, "body", bodyStr)

	// Check for successful switchover
	// 200: Immediate successful switchover
	// 202: Scheduled switchover (we use scheduled_at=now, but might still get 202)
	if resp.StatusCode == 200 || resp.StatusCode == 202 {
		// Like CLI implementation, check for "Successfully" to ensure we got the requested candidate
		// Patroni might switch to a different candidate and return different message format
		if strings.Contains(bodyStr, "Successfully") {
			p.logger.Info("Switchover successful")
			return true, nil
		} else {
			// Patroni chose different candidate - return false like CLI does for consistency
			p.logger.Info("Switchover occurred but not to requested candidate", "response", bodyStr)
			return false, nil
		}
	}

	// Any other status code is an error
	err = fmt.Errorf("switchover failed with status %d: %s", resp.StatusCode, bodyStr)
	p.logger.Error(err, "Failed to switchover")
	return false, err
}

func (p instanceClient) failover(ctx context.Context, candidate string) (bool, error) {
	p.logger.Info("Requesting failover")

	if candidate == "" {
		err := fmt.Errorf("candidate is required for failover")
		p.logger.Error(err, "Failover failed. Candidate is required")
		return false, err
	}

	resp, err := p.do(ctx, http.MethodPost, patroniEndpointFailover, map[string]any{
		"candidate": candidate,
	})

	if err != nil {
		p.logger.Error(err, "Failover failed")
		return false, err
	}

	defer resp.Body.Close()
	body, _ := io.ReadAll(resp.Body)
	bodyStr := string(body)

	p.logger.V(1).Info("failover response", "status", resp.StatusCode, "body", bodyStr)

	// Failover cannot be scheduled, so we don't check for 202.
	if resp.StatusCode == 200 {
		// Like CLI implementation, check for "Successfully" to ensure we got the requested candidate
		// Patroni might failover to a different candidate and return different message format
		if strings.Contains(bodyStr, "Successfully") {
			p.logger.Info("Failover successful")
			return true, nil
		} else {
			// Patroni chose different candidate - return false like CLI does for consistency
			p.logger.Info("Failover occurred but not to requested candidate", "response", bodyStr)
			return false, nil
		}
	}

	// Any other status code is an error
	err = fmt.Errorf("failover failed with status %d: %s", resp.StatusCode, bodyStr)
	p.logger.Error(err, "Failover failed")
	return false, err
}

// A fancier version of the restart API that only restart pending nodes that match a given role.
func (p instanceClient) restartPendingWithRole(ctx context.Context, role string) error {
	p.logger.Info("Requesting instance restart for pending changes", "role", role)

	if role == "" {
		// Patroni's rest API does not require role to be filled, but /our/ usage
		// expects a role check, so we also enforce it here.
		err := fmt.Errorf("role is empty")
		p.logger.Error(err, "Restart failed")
		return err
	}

	requestBody := map[string]any{
		"restart_pending": true,
		"role":            role,
	}

	resp, err := p.do(ctx, http.MethodPost, patroniEndpointRestart, requestBody)
	if err != nil {
		err = fmt.Errorf("failed to execute restart request: %w", err)
		p.logger.Error(err, "Restart failed")
		return err
	}

	defer resp.Body.Close()
	body, _ := io.ReadAll(resp.Body)
	bodyStr := string(body)

	p.logger.V(1).Info("restart response", "status", resp.StatusCode, "body", bodyStr)

	// 200: Restart initiated successfully
	// 202: Restart accepted/scheduled
	// 503: No restart needed (restart conditions not satisfied) - this is OK
	if resp.StatusCode == 200 || resp.StatusCode == 202 {
		p.logger.Info("Restart initiated successfully")
		return nil
	}

	if resp.StatusCode == 503 {
		// This is normal when the node doesn't need restart or already restarted
		p.logger.Info("No restart needed (503)", "body", bodyStr)
		return nil
	}

	// Any other status code is an error
	err = fmt.Errorf("restart failed with status %d: %s", resp.StatusCode, bodyStr)
	p.logger.Error(err, "Restart failed")
	return err
}

func newInstanceClient(
	ctx context.Context,
	parentLogger logr.Logger,
	kubeClient client.Client,
	pod podMetadata,
) (instanceClient, error) {
	logger := parentLogger.WithValues(
		"namespace", pod.Namespace,
		"pod", pod.Name,
	).WithName("client")

	// Fetch client certificates. They are created in the same namespace as
	// the pods with a special "-certs" suffix.
	secret := &corev1.Secret{}
	err := kubeClient.Get(ctx, pod.getSecretCertKey(), secret)
	if err != nil {
		return instanceClient{}, fmt.Errorf("failed to fetch certificates for patroni: %w", err)
	}

	rawCaCert := secret.Data[certAuthorityFileKey]
	rawClientCert := secret.Data[certServerFileKey]

	if len(rawCaCert) == 0 {
		return instanceClient{}, fmt.Errorf("CA certificate is empty")
	}

	if len(rawClientCert) == 0 {
		return instanceClient{}, fmt.Errorf("Client certificate is empty")
	}

	caCert := x509.NewCertPool()
	if !caCert.AppendCertsFromPEM(rawCaCert) {
		return instanceClient{}, fmt.Errorf("failed to parse CA certificate from secret")
	}

	clientCert, err := tls.X509KeyPair(rawClientCert, rawClientCert)
	if err != nil {
		return instanceClient{}, fmt.Errorf(
			"failed to parse key pair from combined cert: %w",
			err,
		)
	}

	httpClient := &http.Client{
		Timeout: 30 * time.Second,
		Transport: otelhttp.NewTransport(&http.Transport{
			TLSClientConfig: &tls.Config{
				RootCAs:      caCert,
				Certificates: []tls.Certificate{clientCert},
			},
		}),
	}

	return instanceClient{
		baseUrl:    pod.getPatroniServerUrl(),
		httpClient: httpClient,
		logger:     logger,
	}, nil
}

type HTTPClient struct {
	kubeClient client.Client
	client     instanceClient
	logger     logr.Logger
	tracer     trace.Tracer
}

var _ API = HTTPClient{}

func NewHttpClient(ctx context.Context, kube client.Client, podName string) (HTTPClient, error) {
	logger := logging.FromContext(ctx).WithName("patroni.http")
	tracer := otel.Tracer("github.com/percona/percona-postgresql-operator/patroni")

	// We can extract all the information we need from the podName due to the
	// way podName is built: ${namespace}-${instanceSuffix}-${podNumeral}.
	podMetadata, err := extractMetadataFromPodName(podName)
	if err != nil {
		return HTTPClient{}, err
	}

	patroniHttpClient, err := newInstanceClient(ctx, logger, kube, podMetadata)

	if err != nil {
		return HTTPClient{}, err
	}

	return HTTPClient{
		client:     patroniHttpClient,
		kubeClient: kube,
		logger:     logger,
		tracer:     tracer,
	}, nil
}

// Called when the operator believes Patroni configuration needs to be updated due to CRD changes.
func (h HTTPClient) ReplaceConfiguration(ctx context.Context, configuration map[string]any) error {
	ctx, span := h.tracer.Start(ctx, "patroni.replace-configuration")
	defer span.End()

	h.logger.Info("Calling ReplaceConfiguration")

	err := h.client.putConfig(ctx, configuration)
	if err != nil {
		span.RecordError(err)
	}
	return err
}

// Called when the operator detects pod restarts or changes that require pod restarts, such
// as CPU/mem changes.
func (h HTTPClient) ChangePrimaryAndWait(
	ctx context.Context,
	leader, candidate string,
	_patroniVer4 bool,
) (bool, error) {
	ctx, span := h.tracer.Start(ctx, "patroni.change-primary")
	defer span.End()

	span.SetAttributes(
		attribute.String("patroni.leader", leader),
		attribute.String("patroni.candidate", candidate),
		attribute.Bool("patroni.ver4", _patroniVer4),
	)

	h.logger.WithValues("leader", leader).Info("Calling ChangePrimaryAndWait")

	success, err := h.client.switchover(ctx, leader, candidate)
	if err != nil {
		span.RecordError(err)
	}
	span.SetAttributes(attribute.Bool("patroni.success", success))
	return success, err
}

// Very similar to ChangePrimaryAndWait, but implemented by the Percona team for
// the reconcileSwitchover method. The difference here is that SwitchoverAndWait
// does not provide a leader.
func (h HTTPClient) SwitchoverAndWait(ctx context.Context, candidate string) (bool, error) {
	ctx, span := h.tracer.Start(ctx, "patroni.switchover")
	defer span.End()

	span.SetAttributes(attribute.String("patroni.candidate", candidate))

	h.logger.WithValues("candidate", candidate).Info("Calling SwitchoverAndWait")
	leader, err := h.client.getLeader(ctx)

	if err != nil {
		h.logger.Error(
			err,
			"Failed to auto-detect current leader for switchover",
		)
		span.RecordError(err)
		return false, fmt.Errorf("failed to detect current leader: %w", err)
	}

	span.SetAttributes(attribute.String("patroni.leader", leader.Name))

	// NOTE:
	// Potential race condition where the leader changes between these two calls.
	// If this happens, Patroni will error out and the operation will be retried.
	h.logger.Info(
		"Auto-detected current leader",
		"leader",
		leader,
		"candidate",
		candidate,
	)

	success, err := h.client.switchover(ctx, leader.Name, candidate)
	if err != nil {
		span.RecordError(err)
	}
	span.SetAttributes(attribute.Bool("patroni.success", success))
	return success, err
}

// FailoverAndWait tries to change the leader when the cluster is NOT healthy. When it's
// healthy, switchover is advised.
// Ref.: https://patroni.readthedocs.io/en/latest/rest_api.html#failover
func (h HTTPClient) FailoverAndWait(ctx context.Context, candidate string) (bool, error) {
	ctx, span := h.tracer.Start(ctx, "patroni.failover")
	defer span.End()

	span.SetAttributes(attribute.String("patroni.candidate", candidate))

	h.logger.WithValues("candidate", candidate).Info("Calling FailoverAndWait")

	success, err := h.client.failover(ctx, candidate)
	if err != nil {
		span.RecordError(err)
	}
	span.SetAttributes(attribute.Bool("patroni.success", success))
	return success, err
}

// Restarts Patroni members that have a pending restart and match a given role.
// The pending status is given by Patroni when it detects that a configuration was updated that
// requires a restart to take effect. The operator watches for the pending status and first
// asks for the leader to be restarted, followed by its replicas.
func (h HTTPClient) RestartPendingMembers(ctx context.Context, role, _scope string) error {
	ctx, span := h.tracer.Start(ctx, "patroni.restart-pending-members")
	defer span.End()

	span.SetAttributes(
		attribute.String("patroni.role", role),
		attribute.String("patroni.scope", _scope),
	)

	h.logger.WithValues("role", role).Info("Calling RestartPendingMembers")

	if role == "" {
		// Role is not a required field for restart, but the operator's usage of it will
		// always pass a role to double check we are restarting nodes in the correct order:
		// first primary, then replicas. To make sure this is the case, we error if the
		// function was called without a role.
		err := fmt.Errorf("role is empty")
		h.logger.Error(err, "Failed to restart pending members")
		span.RecordError(err)
		return err
	}

	roles := []string{role}
	// patronictl allows both "primary" and "leader" roles to mean "leader", so
	// in order to keep compatibility we need to check for both.
	// NOTE(juliana): We can remove this after we fully migrate to the REST API.
	if role == "primary" {
		roles = append(roles, "leader")
	}

	members, err := h.client.getMembersByRole(ctx, roles)
	if err != nil {
		h.logger.Error(err, "Failed to fetch cluster members")
		span.RecordError(err)
		return err
	}

	span.SetAttributes(attribute.Int("patroni.members_found", len(members)))

	if len(members) == 0 {
		h.logger.Info("Found no members to restart", "role", role)
		return nil
	}

	for _, member := range members {
		h.logger.Info("Requesting restart for pod", "pod", member.Name, "role", role)

		podMetadata, err := extractMetadataFromPodName(member.Name)
		if err != nil {
			span.RecordError(err)
			return err
		}

		client, err := newInstanceClient(ctx, h.logger, h.kubeClient, podMetadata)
		if err != nil {
			h.logger.Error(err, "Failed to create client for pod", "pod", member.Name)
			span.RecordError(err)
			return err
		}

		err = client.restartPendingWithRole(ctx, role)
		if err != nil {
			h.logger.Error(err, "Restart failed for pod", "pod", member.Name)
			span.RecordError(err)
			return err
		}
	}

	h.logger.Info("Restart pending members succeeded")
	return nil
}

// Gets the current timeline from Patroni cluster status.
// Used as sanity check by the operator before a switchover/failover operation.
// Returns the timeline of the running leader, or 0 if no running leader found.
func (h HTTPClient) GetTimeline(ctx context.Context) (int64, error) {
	ctx, span := h.tracer.Start(ctx, "patroni.get-timeline")
	defer span.End()

	h.logger.Info("Calling GetTimeline")

	leader, err := h.client.getLeader(ctx)
	if err != nil {
		h.logger.Info("No leader found for timeline", "error", err)
		span.SetAttributes(attribute.Bool("patroni.leader_found", false))
		return 0, nil // Return 0 when no leader (matches CLI behavior)
	}

	span.SetAttributes(
		attribute.Bool("patroni.leader_found", true),
		attribute.String("patroni.leader_name", leader.Name),
		attribute.String("patroni.leader_state", leader.State),
	)

	// Check if leader is running (same logic as CLI implementation)
	if leader.State != "running" {
		h.logger.Info("Leader not in running state", "state", leader.State)
		span.SetAttributes(attribute.Int64("patroni.timeline", 0))
		return 0, nil
	}

	h.logger.Info("Found running leader", "member", leader.Name, "timeline", leader.Timeline)
	span.SetAttributes(attribute.Int64("patroni.timeline", leader.Timeline))
	return leader.Timeline, nil
}
