#!/bin/bash

exec 5>&2
BASH_XTRACEFD="5"

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | sed -e 's/_/-/g' | tr '[:upper:]' '[:lower:]')}
IMAGE_BASE=${IMAGE_BASE:-"perconalab/percona-postgresql-operator"}
IMAGE_URI_BASE=${IMAGE_URI_BASE:-"${IMAGE_BASE}:${GIT_BRANCH}"}
IMAGE_APISERVER=${IMAGE_APISERVER:-"${IMAGE_URI_BASE}-pgo-apiserver"}
IMAGE_PGOEVENT=${IMAGE_PGOEVENT:-"${IMAGE_URI_BASE}-pgo-event"}
IMAGE_RMDATA=${IMAGE_RMDATA:-"${IMAGE_URI_BASE}-pgo-rmdata"}
IMAGE_SCHEDULER=${IMAGE_SCHEDULER:-"${IMAGE_URI_BASE}-pgo-scheduler"}
IMAGE_OPERATOR=${IMAGE_OPERATOR:-"${IMAGE_URI_BASE}-postgres-operator"}
IMAGE_DEPLOYER=${IMAGE_DEPLOYER:-"${IMAGE_URI_BASE}-pgo-deployer"}
IMAGE_PGBOUNCER=${IMAGE_PGBOUNCER:-"${IMAGE_BASE}:main-ppg13-pgbouncer"}
SKIP_BACKUPS_TO_AWS_GCP=${SKIP_BACKUPS_TO_AWS_GCP:-1}
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)

if [ -f "$conf_dir/cloud-secret.yml" ]; then
	SKIP_BACKUPS_TO_AWS_GCP=''
fi

create_namespace() {
	local namespace="$1"
	local skip_clean_namespace="$2"

	if [[ ${CLEAN_NAMESPACE} == 1 ]] && [[ -z ${skip_clean_namespace} ]]; then
		kubectl_bin get ns \
			| egrep -v "^kube-|^default|Terminating|openshift|^NAME" \
			| awk '{print$1}' \
			| xargs kubectl delete ns &
		kubectl delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
		kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	fi

	if [ -n "${OPENSHIFT}" ]; then
		oc delete project "$namespace" && sleep 40 || :
		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl_bin delete namespace "$namespace" || :
		wait_for_delete "namespace/$namespace"
		kubectl_bin create namespace "$namespace"
		kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
	fi
}

desc() {
	set +o xtrace
	local msg="$@"
	printf "\n\n-----------------------------------------------------------------------------------\n"
	printf "$msg"
	printf "\n-----------------------------------------------------------------------------------\n\n"
	set -o xtrace
}

deploy_operator() {
	desc 'start operator'

	# modifing ini structure from configMap
	yq r -d'2' ${src_dir}/deploy/operator.yaml 'data[values.yaml]' \
		| $sed -e "s#namespace: .*#namespace: ${namespace}#g" \
		| $sed -e "s#pgo_operator_namespace: .*#pgo_operator_namespace: ${namespace}#g" \
		| $sed -e "s#pgo_image_tag: .*#pgo_image_tag: ${GIT_BRANCH}#g" \
			>${tmp_dir}/operator.ini

	# updating yaml itself
	yq w -d'*' ${src_dir}/deploy/operator.yaml 'metadata.namespace' ${namespace} \
		| yq w -d'3' - 'subjects[0].namespace' ${namespace} \
		| yq w -d'4' - 'spec.template.spec.containers[0].image' ${IMAGE_DEPLOYER} \
		| yq w -d'2' - -d2 'data[values.yaml]' "$(cat ${tmp_dir}/operator.ini)" \
		| kubectl_bin apply -f -

	wait_pod_completion $(kubectl_bin get pod --selector=job-name=pgo-deploy -o 'jsonpath={.items[].metadata.name}')

	wait_pod $(get_operator_pod)
}

get_operator_pod() {
	kubectl_bin get pods \
		--selector=name=postgres-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

wait_pod_completion() {
	local pod=$1
	local target_phase=${2:-"Succeeded"}

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep "${target_phase}"; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

apply_cluster() {
	local path=${1}
	local name=${2}
	local podAntiAffinity=${3}
	yq w "${path}" 'metadata.namespace' ${namespace} \
		| yq w - 'spec.namespace' ${namespace} \
		| yq w - 'metadata.annotations.current-primary' ${name} \
		| yq w - 'metadata.labels.crunchy-pgha-scope' ${name} \
		| yq w - 'metadata.labels.deployment-name' ${name} \
		| yq w - 'metadata.labels.name' ${name} \
		| yq w - 'metadata.labels.pg-cluster' ${name} \
		| yq w - 'metadata.name' ${name} \
		| yq w - 'spec.PrimaryStorage.name' ${name} \
		| yq w - 'spec.clustername' ${name} \
		| yq w - 'spec.database' ${name} \
		| yq w - 'spec.name' ${name} \
		| yq w - 'spec.user' ${name} \
		| yq d - 'spec.pgBouncer' \
			>${tmp_dir}/cr.yml

	case ${podAntiAffinity} in
		'preferred')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		'required')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		'disabled')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		*) ;;
	esac

	kubectl_bin apply -f ${tmp_dir}/cr.yml
}

kubectl_bin() {
	local LAST_OUT="$(mktemp)"
	local LAST_ERR="$(mktemp)"
	local exit_status=0
	local timeout=4
	for i in $(seq 0 2); do
		kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
		exit_status=$?
		[[ ${-/x/} != $- ]] && echo "--- $i stdout" | cat - "$LAST_OUT" >&$BASH_XTRACEFD
		[[ ${-/x/} != $- ]] && echo "--- $i stderr" | cat - "$LAST_ERR" >&$BASH_XTRACEFD
		if [[ ${exit_status} != 0 ]]; then
			sleep "$((timeout * i))"
		else
			cat "$LAST_OUT"
			cat "$LAST_ERR" >&2
			rm "$LAST_OUT" "$LAST_ERR"
			return ${exit_status}
		fi
	done
	cat "$LAST_OUT"
	cat "$LAST_ERR" >&2
	rm "$LAST_OUT" "$LAST_ERR"
	return ${exit_status}
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_deployment() {
	local name=$1

	sleep 10
	set +o xtrace
	retry=0
	echo -n $name
	until [ "$(kubectl_bin get deployment $name -o jsonpath='{.status.replicas}')" == "$(kubectl_bin get deployment $name -o jsonpath='{.status.readyReplicas}')" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

enable_pgBouncer() {
	local cluster_name=${1}
	local replicas_num=${2:-'1'}

	kubectl_bin patch \
		"pgcluster/${cluster_name}" \
		--type json \
		-p='[{"op":"add","path":"/spec/pgBouncer","value":{"image":"'"${IMAGE_PGBOUNCER}"'","limits":{"cpu":"2","memory":"512Mi"},"replicas":'${replicas_num}',"resources":{"cpu":"1","memory":"128Mi"},"tlsSecret":"","serviceType":"LoadBalancer"}}]'
}

disable_pgbouncer() {
	kubectl_bin patch \
		"pgcluster/${1}" \
		--type json \
		-p='[{"op":"remove","path":"/spec/pgBouncer"}]'
}

compare_kubectl() {
	local resource="$1"
	local postfix="$2"
	local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
	local new_result="${tmp_dir}/${resource//\//_}.yml"

	kubectl_bin get -o yaml ${resource} \
		| yq d - 'metadata.managedFields' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.namespace' \
		| yq d - '**.uid' \
		| yq d - 'metadata.resourceVersion' \
		| yq d - 'metadata.selfLink' \
		| yq d - 'metadata.deletionTimestamp' \
		| yq d - 'metadata.annotations."k8s.v1.cni.cncf.io*"' \
		| yq d - 'metadata.annotations."kubernetes.io/psp"' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.image' \
		| yq d - '**.clusterIP' \
		| yq d - '**.dataSource' \
		| yq d - '**.procMount' \
		| yq d - '**.storageClassName' \
		| yq d - '**.finalizers' \
		| yq d - '**."kubernetes.io/pvc-protection"' \
		| yq d - '**.volumeName' \
		| yq d - '**."volume.beta.kubernetes.io/storage-provisioner"' \
		| yq d - 'spec.volumeMode' \
		| yq d - 'spec.nodeName' \
		| yq d - '**."volume.kubernetes.io/selected-node"' \
		| yq d - '**."percona.com/*"' \
		| yq d - '**.(volumeMode==Filesystem).volumeMode' \
		| yq d - '**.healthCheckNodePort' \
		| yq d - '**.nodePort' \
		| yq d - '**.imagePullSecrets' \
		| yq d - '**.enableServiceLinks' \
		| yq d - 'status' \
		| yq d - '**.(name==suffix)' \
		| yq d - '**.(name==NAMESPACE)' \
		| yq d - 'spec.volumeClaimTemplates.*.apiVersion' \
		| yq d - 'spec.volumeClaimTemplates.*.kind' \
		| yq d - 'metadata.ownerReferences.*.apiVersion' \
		| yq d - '**.controller-uid' \
		| yq d - '**.preemptionPolicy' \
			>${new_result}

	diff -u ${expected_result} ${new_result}
}

spinup_pgcluster() {
	local cluster=$1
	local config=$2
	local podAntiAffinity=${3}

	desc 'create first PG cluster'

	yq r -d0 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-pgbouncer-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d1 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-postgres-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d2 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-primaryuser-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d3 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-${cluster}-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -

	apply_cluster ${config} ${cluster} ${podAntiAffinity}

	wait_cluster_consistency ${cluster}

	desc 'write data'

	run_psql \
		'CREATE DATABASE myapp; \c myapp \\\ CREATE TABLE IF NOT EXISTS myApp (id int PRIMARY KEY);' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ INSERT INTO myApp (id) VALUES (100500)' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ GRANT SELECT ON myApp to "some-name";GRANT USAGE ON SCHEMA public TO "some-name";' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	sleep 10
}

wait_cluster_consistency() {
	cluster_name=${1}
	retry=0

	wait_deployment ${1}
	wait_deployment "${1}-backrest-shared-repo"

	until [[ "$(kubectl_bin get Pgcluster "${cluster_name}" -o jsonpath='{.status.state}')" == "pgcluster Initialized" ]]; do
		let retry+=1
		if [ $retry -ge 16 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
		echo 'waiting for cluster readyness'
		sleep 10
	done
}

get_client_pod() {
	kubectl_bin get pods \
		--selector=name=pg-client \
		-o 'jsonpath={.items[].metadata.name}'
}

compare_psql_cmd() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local postfix="$4"
	local uri_suffix="${5}"

	local expected_result=${test_dir}/compare/${command_id}${postfix}.sql

	run_psql "$command" "$uri" "postgres" "$uri_suffix" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql "$command" "$uri" "postgres" "$uri_suffix" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

run_psql() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://$uri$suffix"
}

destroy() {
	local namespace="$1"

	kubectl_bin logs $(get_operator_pod) \
		| grep -v 'level=info' \
		| grep -v 'level=debug' \
		| grep -v 'Getting tasks for pod' \
		| grep -v 'Getting pods from source' \
		| grep -v 'the object has been modified' \
		| grep -v 'get backup status: Job.batch' \
		| $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
		| sort -u \
		| tee $tmp_dir/operator.log

	if [ -n "$OPENSHIFT" ]; then
		oc delete --grace-period=0 --force=true project "$namespace"
	else
		kubectl_bin delete --grace-period=0 --force=true namespace "$namespace"
	fi
	kubectl_bin delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
	kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	rm -rf ${tmp_dir}
}

deploy_cert_manager() {
    kubectl_bin create namespace cert-manager || :
    kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
    kubectl_bin apply -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml --validate=false || : 2>/dev/null
    sleep 45
}

create_replica() {
	local cluster=${1}
	local suffix=${2}

	yq w ${conf_dir}/some-name-replica.yml 'metadata.labels.name' "${cluster}-${suffix}" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${cluster}-${suffix}" \
		| yq w - 'spec.clustername' "${cluster}" \
		| yq w - 'spec.name' "${cluster}-${suffix}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.replicastorage.name' "${cluster}-${suffix}" \
		| kubectl_bin apply -f -
}

check_replica() {
	local name=${1}
	local cluster=${2}
	local sql_cmp=${3:-"select-2"}

	run_psql \
		'SELECT usename,application_name,client_addr,state from pg_stat_replication' \
		"postgres:postgres_pass@${cluster}.${namespace}" \
		>${tmp_dir}/replicas.list

	replica_pod_name=$(kubectl_bin get pods --selector=replica-name=${name} -o 'jsonpath={.items[0].metadata.name}')
	if [[ -z "$(grep ${replica_pod_name} ${tmp_dir}/replicas.list | grep "streaming")" ]]; then
		echo "${replica_pod_name} is not connected or has valid data. Exiting..."
		cat "${tmp_dir}/replicas.list"
		exit 1
	fi
	replica_pod_IP=$(kubectl_bin get pods --selector=replica-name=${name} -o 'jsonpath={.items[0].status.podIP}')
	compare_psql_cmd \
		"${sql_cmp}" \
		'\c myapp \\\ SELECT * from myApp;' \
		"some-name:some-name_pass@${replica_pod_IP}" '' ' '
}
