#!/bin/bash

exec 5>&2
BASH_XTRACEFD="5"

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | sed -e 's/_/-/g' | tr '[:upper:]' '[:lower:]')}
IMAGE_BASE=${IMAGE_BASE:-"perconalab/percona-postgresql-operator"}
IMAGE_URI_BASE=${IMAGE_URI_BASE:-"${IMAGE_BASE}:${GIT_BRANCH}"}
IMAGE_APISERVER=${IMAGE_APISERVER:-"${IMAGE_URI_BASE}-pgo-apiserver"}
IMAGE_PGOEVENT=${IMAGE_PGOEVENT:-"${IMAGE_URI_BASE}-pgo-event"}
IMAGE_RMDATA=${IMAGE_RMDATA:-"${IMAGE_URI_BASE}-pgo-rmdata"}
IMAGE_SCHEDULER=${IMAGE_SCHEDULER:-"${IMAGE_URI_BASE}-pgo-scheduler"}
IMAGE_OPERATOR=${IMAGE_OPERATOR:-"${IMAGE_URI_BASE}-postgres-operator"}
IMAGE_DEPLOYER=${IMAGE_DEPLOYER:-"${IMAGE_URI_BASE}-pgo-deployer"}
IMAGE_PGBOUNCER=${IMAGE_PGBOUNCER:-"${IMAGE_BASE}:main-ppg13-pgbouncer"}
IMAGE_PG_HA=${IMAGE_PG_HA:-"${IMAGE_BASE}:main-ppg13-postgres-ha"}
IMAGE_BACKREST=${IMAGE_BACKREST:-"${IMAGE_BASE}:main-ppg13-pgbackrest"}
IMAGE_BACKREST_REPO=${IMAGE_BACKREST_REPO:-"${IMAGE_BASE}:main-ppg13-pgbackrest-repo"}
IMAGE_PGBADGER=${IMAGE_PGBADGER:-"${IMAGE_BASE}:main-ppg13-pgbadger"}
SKIP_BACKUPS_TO_AWS_GCP=${SKIP_BACKUPS_TO_AWS_GCP:-1}
BUCKET=${BUCKET:-"pg-operator-testing"}
PGO_K8S_NAME=${PGO_K8S_NAME}
ECR=${ECR:-"119175775298.dkr.ecr.us-east-1.amazonaws.com"}

tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)

if [ -f "$conf_dir/cloud-secret.yml" ]; then
	SKIP_BACKUPS_TO_AWS_GCP=''
fi

if oc get projects; then
	OPENSHIFT=4
fi

KUBE_VERSION=$(kubectl version -o json | jq -r '.serverVersion.major + "." + .serverVersion.minor' | $sed -r 's/[^0-9.]+//g')

HELM_VERSION=$(helm version -c | $sed -re 's/.*SemVer:"([^"]+)".*/\1/; s/.*\bVersion:"([^"]+)".*/\1/')
if [ "${HELM_VERSION:0:2}" == "v2" ]; then
	HELM_ARGS="--name"
fi

version_gt() {
	# return true if kubernetes version equal or greater than desired
	if [ $(echo "${KUBE_VERSION} >= $1" | bc -l) -eq 1 ]; then
		return 0
	else
		return 1
	fi
}

cleanup_rbac() {
	kubectl_bin delete clusterrolebindings \
		pgo-cluster-role pgo-deployer-cr \
		chaos-mesh-chaos-controller-manager-cluster-level || true
	kubectl_bin delete clusterroles \
		pgo-cluster-role pgo-deployer-cr \
		chaos-mesh-chaos-controller-manager-target-namespace \
		chaos-mesh-chaos-controller-manager-cluster-level || true
}

create_namespace() {
	local namespace="$1"
	local skip_clean_namespace="$2"

	if [[ ${CLEAN_NAMESPACE} == 1 ]] && [[ -z ${skip_clean_namespace} ]]; then
		kubectl_bin get ns \
			| egrep -v "^kube-|^default|Terminating|openshift|^NAME" \
			| awk '{print$1}' \
			| xargs kubectl delete ns &
		cleanup_rbac
		kubectl delete MutatingWebhookConfiguration/chaos-mesh-mutation \
			ValidatingWebhookConfiguration/chaos-mesh-validation \
			ValidatingWebhookConfiguration/validate-auth || true
	fi

	if [ -n "${OPENSHIFT}" ]; then
		oc delete project "$namespace" && sleep 40 || :
		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl_bin delete namespace "$namespace" || :
		wait_for_delete "namespace/$namespace"
		kubectl_bin create namespace "$namespace"
		kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
	fi
}

deploy_cert_manager() {
	kubectl_bin create namespace cert-manager || :
	kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl_bin apply -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml --validate=false || : 2>/dev/null
	sleep 30
}

desc() {
	set +o xtrace
	local msg="$@"
	printf "\n\n-----------------------------------------------------------------------------------\n"
	printf "$msg"
	printf "\n-----------------------------------------------------------------------------------\n\n"
	set -o xtrace
}

deploy_operator() {
	desc 'start operator'
	local pull_secret_name=${1}

	# modifing ini structure from configMap
	yq r -d'2' ${src_dir}/deploy/operator.yaml 'data[values.yaml]' \
		| $sed -e "s#namespace: .*#namespace: \"${namespace}\"#g" \
		| $sed -e "s#pgo_operator_namespace: .*#pgo_operator_namespace: \"${namespace}\"#g" \
		| $sed -e "s#pgo_image_tag: .*#pgo_image_tag: $(echo ${IMAGE_OPERATOR%-postgres-operator} | cut -d: -f2)#g" \
		| $sed -e "s#ccp_image_prefix: .*#ccp_image_prefix: \"$(echo ${IMAGE_OPERATOR%-postgres-operator} | cut -d: -f1)\"#g" \
		| $sed -e "s#pgo_image_prefix: .*#pgo_image_prefix: \"$(echo ${IMAGE_OPERATOR%-postgres-operator} | cut -d: -f1)\"#g" \
			>${tmp_dir}/operator.ini

	if [[ -n ${pull_secret_name} ]]; then
		$sed -i ${tmp_dir}/operator.ini -e "s#ccp_image_pull_secret: .*#ccp_image_pull_secret: \"${pull_secret_name}\"#g"
		$sed -i ${tmp_dir}/operator.ini -e "s#pgo_image_pull_secret: .*#pgo_image_pull_secret: \"${pull_secret_name}\"#g"
	fi

	if [ -n "$OPENSHIFT" ]; then
		$sed -i -e 's#disable_fsgroup: .*#disable_fsgroup: "true"#g' ${tmp_dir}/operator.ini
	fi

	# updating yaml itself
	yq w -d'*' ${src_dir}/deploy/operator.yaml 'metadata.namespace' ${namespace} \
		| yq w -d'3' - 'subjects[0].namespace' ${namespace} \
		| yq w -d'4' - 'spec.template.spec.containers[0].image' ${IMAGE_DEPLOYER} \
		| yq w -d'2' - -d2 'data[values.yaml]' "$(cat ${tmp_dir}/operator.ini)" \
			>${tmp_dir}/operator.yaml

	if [[ -n ${pull_secret_name} ]]; then
		yq w -i -d'0' "${tmp_dir}/operator.yaml" 'imagePullSecrets[0].name' "${pull_secret_name}"
		yq w -i -d'4' "${tmp_dir}/operator.yaml" 'spec.template.spec.imagePullSecrets[0].name' "${pull_secret_name}"
	fi
	kubectl_bin apply -f ${tmp_dir}/operator.yaml

	wait_job_completion "pgo-deploy"

	wait_pod $(get_operator_pod)
}

deploy_miniogw() {
	local gsc_projectid=${1:-"cloud-dev-112233"}
	kubectl_bin apply -f $conf_dir/cloud-secret-minio-gw.yml \
		-f $src_dir/deploy/backup/minio-gw-tls.yaml

	yq w -d0 --style=single $src_dir/deploy/backup/minio-gw.yaml 'spec.template.spec.containers[0].args[4]' "${gsc_projectid}" \
		| kubectl_bin apply -f -

	wait_pod "$(kubectl_bin get pod --selector=app=minio-gw -o 'jsonpath={.items[].metadata.name}')"
}

get_operator_pod() {
	kubectl_bin get pods \
		--selector=name=postgres-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

wait_pod_completion() {
	local pod=$1
	local target_phase=${2:-"Succeeded"}

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep "${target_phase}"; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_job_completion() {
	local job=$1
	local mandatory=${2:-"true"}

	retry=0
	set +o xtrace
	until KUBECONFIG=${TARGET_CONFIG:-"${KUBECONFIG}"} kubectl get job/${job} -o jsonpath='{.metadata.name}' 2>/dev/null; do
		sleep 1
		echo -n .
		let retry+=1
		if [[ $retry -ge 30 && ${mandatory} == "true" ]]; then
			echo max retry count $retry reached. No target object found.
			exit 1
		elif [[ $retry -ge 30 && ${mandatory} != "true" ]]; then
			echo Can not detect job. Passing by.
			set -o xtrace
			return 0
		fi
	done

	retry=0
	until [[ $(kubectl_bin get job/$job -o jsonpath='{.metadata.name}' 2>&1 | grep -io 'not found') == "not found" ]] \
		|| [[ $(kubectl_bin get job/$job -o jsonpath='{.status.succeeded}' 2>/dev/null) == '1' ]]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 600 ]; then
			kubectl_bin describe job/$job
			kubectl_bin logs $(kubectl_bin get pods --selector=job-name=${job} -o jsonpath='{.items[0].metadata.name}')
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

apply_cluster() {
	local path=${1}
	local name=${2}
	local backup=${3:-"false"}
	local restore_from=${4:-"false"}
	local restore_opts=${5:-"false"}
	local custom_config=${6:-"false"}
	local podAntiAffinity=${7}

	yq w "${path}" 'metadata.namespace' ${namespace} \
		| yq w - 'spec.namespace' ${namespace} \
		| yq w - 'metadata.annotations.current-primary' ${name} \
		| yq w - 'metadata.labels.crunchy-pgha-scope' ${name} \
		| yq w - 'metadata.labels.deployment-name' ${name} \
		| yq w - 'metadata.labels.name' ${name} \
		| yq w - 'metadata.labels.pg-cluster' ${name} \
		| yq w - 'metadata.name' ${name} \
		| yq w - 'spec.clustername' ${name} \
		| yq w - 'spec.database' ${name} \
		| yq w - 'spec.name' ${name} \
		| yq w - 'spec.user' ${name} \
		| yq w - 'spec.pgPrimary.image' ${IMAGE_PG_HA} \
		| yq w - 'spec.backup.image' ${IMAGE_BACKREST} \
		| yq w - 'spec.backup.backrestRepoImage' ${IMAGE_BACKREST_REPO} \
		| yq w - 'spec.pgBadger.image' ${IMAGE_PGBADGER} \
		| yq w - 'spec.pgBouncer.image' ${IMAGE_PGBOUNCER} \
			>"${tmp_dir}/cr.yaml"


	case ${podAntiAffinity} in
		'preferred')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		'required')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		'disabled')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		*) ;;
	esac

	case ${backup} in
		'local')
			yq w -i "${tmp_dir}/cr.yaml" 'spec.backup.storageTypes[0]' 'local'
			;;
		'gcs')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storageTypes[0]' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'gcs+'*)
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storageTypes[0]' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
				| yq w - 'spec.backup.repoPath' "/backrestrepo/${backup//'gcs+'/}-backrest-shared-repo" \
				| yq w - 'spec.standby' true \
				| yq d - 'spec.pgBouncer' \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		's3')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
				| yq w - 'spec.backup.storageTypes[0]' 's3' \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'local,s3')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
				| yq w - 'spec.backup.storageTypes[0]' 'local' \
				| yq w - 'spec.backup.storageTypes[1]' 's3' \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'local,gcs')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storageTypes[0]' 'local' \
				| yq w - 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storageTypes[1]' 'gcs' \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		*) ;;
	esac

	if [[ ${restore_from} != "false" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreFrom' "${restore_from}" \
			>"${tmp_dir}/cr.restore.yaml"
		mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		if [[ ${restore_opts} != "false" ]]; then
			yq w --style=single "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreOpts' "${restore_opts}" \
				>"${tmp_dir}/cr.restore.yaml"
			mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		fi
	fi

	if [[ ${custom_config} != "false" ]]; then
		yq w -i "${tmp_dir}/cr.yaml" 'spec.customconfig' ${custom_config}
	fi

	kubectl_bin apply -f "${tmp_dir}/cr.yaml"
}

kubectl_bin() {
	local LAST_OUT="$(mktemp)"
	local LAST_ERR="$(mktemp)"
	local exit_status=0
	local timeout=4
	for i in $(seq 0 2); do
		KUBECONFIG=${TARGET_CONFIG:-"${KUBECONFIG}"} kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
		exit_status=$?
		[[ ${-/x/} != $- ]] && echo "--- $i stdout" | cat - "$LAST_OUT" >&$BASH_XTRACEFD
		[[ ${-/x/} != $- ]] && echo "--- $i stderr" | cat - "$LAST_ERR" >&$BASH_XTRACEFD
		if [[ ${exit_status} != 0 ]]; then
			sleep "$((timeout * i))"
		else
			cat "$LAST_OUT"
			cat "$LAST_ERR" >&2
			rm "$LAST_OUT" "$LAST_ERR"
			return ${exit_status}
		fi
	done
	cat "$LAST_OUT"
	cat "$LAST_ERR" >&2
	rm "$LAST_OUT" "$LAST_ERR"
	return ${exit_status}
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_deployment() {
	local name=$1

	sleep 10
	set +o xtrace
	retry=0
	echo -n $name
	until [ "$(kubectl_bin get deployment $name -o jsonpath='{.status.replicas}')" == "$(kubectl_bin get deployment $name -o jsonpath='{.status.readyReplicas}')" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

enable_pgBouncer() {
	local cluster_name=${1}
	local replicas_num=${2:-'1'}

	kubectl_bin patch \
		"pgcluster/${cluster_name}" \
		--type json \
		-p='[{"op":"add","path":"/spec/pgBouncer","value":{"image":"'"${IMAGE_PGBOUNCER}"'","limits":{"cpu":"2","memory":"512Mi"},"replicas":'${replicas_num}',"resources":{"cpu":"1","memory":"128Mi"},"tlsSecret":"","serviceType":"LoadBalancer"}}]'
}

disable_pgbouncer() {
	kubectl_bin patch \
		"pgcluster/${1}" \
		--type json \
		-p='[{"op":"remove","path":"/spec/pgBouncer"}]'
}

compare_kubectl() {
	local resource="$1"
	local postfix="$2"
	local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
	local new_result="${tmp_dir}/${resource//\//_}.yml"

	if [ -n "$OPENSHIFT" -a -f ${expected_result//.yml/-oc.yml} ]; then
		expected_result=${expected_result//.yml/-oc.yml}
	fi

	kubectl_bin get -o yaml ${resource} \
		| yq d - 'metadata.managedFields' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.namespace' \
		| yq d - '**.uid' \
		| yq d - 'metadata.resourceVersion' \
		| yq d - 'metadata.selfLink' \
		| yq d - 'metadata.deletionTimestamp' \
		| yq d - 'metadata.annotations."k8s.v1.cni.cncf.io*"' \
		| yq d - 'metadata.annotations."kubernetes.io/psp"' \
		| yq d - 'metadata.annotations."cloud.google.com/neg"' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.image' \
		| yq d - '**.clusterIP' \
		| yq d - '**.clusterIPs' \
		| yq d - '**.dataSource' \
		| yq d - '**.procMount' \
		| yq d - '**.storageClassName' \
		| yq d - '**.finalizers' \
		| yq d - '**."kubernetes.io/pvc-protection"' \
		| yq d - '**.volumeName' \
		| yq d - '**."volume.beta.kubernetes.io/storage-provisioner"' \
		| yq d - 'spec.volumeMode' \
		| yq d - 'spec.nodeName' \
		| yq d - '**."volume.kubernetes.io/selected-node"' \
		| yq d - '**."percona.com/*"' \
		| yq d - '**.(volumeMode==Filesystem).volumeMode' \
		| yq d - '**.healthCheckNodePort' \
		| yq d - '**.nodePort' \
		| yq d - '**.imagePullSecrets' \
		| yq d - '**.enableServiceLinks' \
		| yq d - 'status' \
		| yq d - '**.(name==suffix)' \
		| yq d - '**.(name==NAMESPACE)' \
		| yq d - 'spec.volumeClaimTemplates.*.apiVersion' \
		| yq d - 'spec.volumeClaimTemplates.*.kind' \
		| yq d - 'metadata.ownerReferences.*.apiVersion' \
		| yq d - '**.controller-uid' \
		| yq d - '**.preemptionPolicy' \
		| yq d - '**.ipFamilies' \
		| yq d - '**.ipFamilyPolicy' \
			>${new_result}

	diff -u ${expected_result} ${new_result}
}

spinup_pgcluster() {
	local cluster=$1
	local config=$2
	local backup=${3:-"false"}
	local custom_config=${4:-"false"}
	local podAntiAffinity=${5}

	desc 'create first PG cluster'

	create_user_secrets "${cluster}"

	desc 'create fresh PG cluster'
	apply_cluster ${config} ${cluster} ${backup} 'false' 'false' ${custom_config} ${podAntiAffinity}
	wait_deployment "${cluster}-backrest-shared-repo"
	wait_deployment "${cluster}"
	# Exiting since we are running cluster in stanby mode
	[[ ${backup} == 'gcs+'* ]] && return

	wait_job_completion "${cluster}-stanza-create" 'false'
	wait_job_completion "backrest-backup-${cluster}"

	wait_cluster_consistency ${cluster}
    sleep 10
	desc 'write data'

	run_psql \
		'CREATE DATABASE myapp; \c myapp \\\ CREATE TABLE IF NOT EXISTS myApp (id int PRIMARY KEY);' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ INSERT INTO myApp (id) VALUES (100500)' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ GRANT SELECT,INSERT ON myApp to "some-name";GRANT USAGE ON SCHEMA public TO "some-name";' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	sleep 10
}

create_user_secrets() {
	local cluster=$1

	yq r -d0 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-pgbouncer-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d1 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-postgres-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d2 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-primaryuser-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d3 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-${cluster}-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
}

wait_cluster_consistency() {
	cluster_name=${1}
	retry=0

	until [[ "$(kubectl_bin get Pgcluster "${cluster_name}" -o jsonpath='{.status.state}')" == "pgcluster Initialized" ]]; do
		let retry+=1
		if [ $retry -ge 16 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
		echo 'waiting for cluster readyness'
		sleep 10
	done
}

get_client_pod() {
	kubectl_bin get pods \
		--selector=name=pg-client \
		-o 'jsonpath={.items[].metadata.name}'
}

compare_psql_cmd() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local postfix="$4"
	local uri_suffix="${5}"

	local expected_result=${test_dir}/compare/${command_id}${postfix}.sql

	run_psql "$command" "$uri" "postgres" "$uri_suffix" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql "$command" "$uri" "postgres" "$uri_suffix" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

run_psql() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://$uri$suffix"
}

destroy() {
	local namespace="$1"

	kubectl_bin logs $(get_operator_pod) \
		| grep -v 'level=info' \
		| grep -v 'level=debug' \
		| grep -v 'Getting tasks for pod' \
		| grep -v 'Getting pods from source' \
		| grep -v 'the object has been modified' \
		| grep -v 'get backup status: Job.batch' \
		| $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
		| sort -u \
		| tee $tmp_dir/operator.log

	kubectl_bin delete -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml 2>/dev/null || :

	if [ -n "$OPENSHIFT" ]; then
		oc delete --grace-period=0 --force=true project "$namespace"
	else
		kubectl_bin delete --grace-period=0 --force=true namespace "$namespace"
	fi
	kubectl_bin delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
	kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	rm -rf ${tmp_dir}
}

deploy_cert_manager() {
	kubectl_bin create namespace cert-manager || :
	kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl_bin apply -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml --validate=false || : 2>/dev/null
	sleep 45
}

create_replica() {
	local cluster=${1}
	local suffix=${2}

	yq w ${conf_dir}/some-name-replica.yml 'metadata.labels.name' "${cluster}-${suffix}" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${cluster}-${suffix}" \
		| yq w - 'spec.clustername' "${cluster}" \
		| yq w - 'spec.name' "${cluster}-${suffix}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.replicastorage.name' "${cluster}-${suffix}" \
		| kubectl_bin apply -f -
}

check_replica() {
	local name=${1}
	local cluster=${2}
	local sql_cmp=${3:-"select-2"}

	run_psql \
		'SELECT usename,application_name,client_addr,state from pg_stat_replication' \
		"postgres:postgres_pass@${cluster}.${namespace}" \
		>${tmp_dir}/replicas.list

	replica_pod_name=$(kubectl_bin get pods --selector=deployment-name=${name},name=${cluster}-replica -o 'jsonpath={.items[0].metadata.name}')
	if [[ -z "$(grep ${replica_pod_name} ${tmp_dir}/replicas.list | grep "streaming")" ]]; then
		echo "${replica_pod_name} is not connected or has valid data. Exiting..."
		cat "${tmp_dir}/replicas.list"
		exit 1
	fi
	replica_pod_IP=$(kubectl_bin get pods --selector=deployment-name=${name},name=${cluster}-replica -o 'jsonpath={.items[0].status.podIP}')
	compare_psql_cmd \
		"${sql_cmp}" \
		'\c myapp \\\ SELECT * from myApp;' \
		"some-name:some-name_pass@${replica_pod_IP}" '' ' '
}

create_backup() {
	local cluster=${1}
	local bckp_prefix=${2}
	local bckp_type=${3:-"full"}

	yq w ${conf_dir}/backup.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.job-name' "${bckp_prefix}-${cluster}" \
		| yq w --style=single -- - 'spec.parameters.backrest-opts' "--type=${bckp_type}" \
		| yq w - 'spec.parameters.pg-cluster' "${cluster}" \
		| yq w - 'spec.parameters.podname' $(kubectl_bin get pods --selector=name=${cluster}-backrest-shared-repo,pg-cluster=${cluster} -o 'jsonpath={.items[].metadata.name}') \
		| kubectl_bin apply -f -
	sleep 10

	wait_job_completion "${bckp_prefix}-${cluster}"
}

run_restore() {
	local cluster=${1}
	local rstr_prefix=${2}
	local storage=${3:-"posix"}
	local target=${4:-"null"} #time, xid or name
	local type=${5:-"time"}

	yq w ${test_dir}/conf/restore.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.backrest-restore-from-cluster' "${cluster}" \
		| yq w - 'spec.parameters.backrest-storage-type' "${storage}" \
			>${tmp_dir}/restore.yml

	if [[ ${target_time} != "null" ]]; then
		yq w ${tmp_dir}/restore.yml --style=single 'spec.parameters.backrest-pitr-target' "${target}" \
			| yq w --style=single -- - 'spec.parameters.backrest-restore-opts' "--type=${type}" \
				>${tmp_dir}/restore.pitr.yml
		mv ${tmp_dir}/restore.pitr.yml ${tmp_dir}/restore.yml
	fi

	kubectl apply -f ${tmp_dir}/restore.yml

	wait_bootstap_completeness ${cluster}
}

wait_bootstap_completeness() {
	local cluster=$1

	wait_job_completion "${cluster}-bootstrap"
	wait_deployment "${cluster}-backrest-shared-repo"
	wait_deployment "${cluster}"
	wait_job_completion "${cluster}-stanza-create" 'false'
	wait_job_completion "backrest-backup-${cluster}"
}

get_service_endpoint() {
	local service=$1

	local hostname=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].hostname' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$hostname" -a "$hostname" != "null" ]; then
		echo $hostname
		return
	fi

	local ip=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].ip' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$ip" -a "$ip" != "null" ]; then
		echo $ip
		return
	fi

	exit 1
}

deploy_helm() {
	helm repo add hashicorp https://helm.releases.hashicorp.com
	helm repo add percona https://percona-charts.storage.googleapis.com/
	helm repo add minio https://helm.min.io/
	helm repo update
}

deploy_chaos_mesh() {
	local chaos_mesh_ns=$1

	desc 'install chaos-mesh'
	local old_cm_namespace=$(helm list --all-namespaces --filter chaos-mesh | tail -n1 | awk -F' ' '{print $2}')
	if [ "${old_cm_namespace}" != "NAMESPACE" ]; then
		helm del chaos-mesh --namespace ${old_cm_namespace} || :
	fi
	helm repo add chaos-mesh https://charts.chaos-mesh.org
	if version_gt "1.19"; then
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${chaos_mesh_ns} --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --set dashboard.create=false --version v2.0.0 --set clusterScoped=false --set controllerManager.targetNamespace=${chaos_mesh_ns}
	else
		helm install chaos-mesh chaos-mesh/chaos-mesh --namespace=${chaos_mesh_ns} --set dashboard.create=false --version v2.0.0 --set clusterScoped=false --set controllerManager.targetNamespace=${chaos_mesh_ns}
	fi
	sleep 10
}

destroy_chaos_mesh() {
	local chaos_mesh_ns=$1

	desc 'destroy chaos-mesh'
	helm del chaos-mesh --namespace ${chaos_mesh_ns} || :
}
