#!/bin/bash

exec 5>&2
BASH_XTRACEFD="5"

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | sed -e 's/_/-/g' | tr '[:upper:]' '[:lower:]')}
IMAGE_BASE=${IMAGE_BASE:-"perconalab/percona-postgresql-operator"}
IMAGE_URI_BASE=${IMAGE_URI_BASE:-"${IMAGE_BASE}:${GIT_BRANCH}"}
IMAGE_APISERVER=${IMAGE_APISERVER:-"${IMAGE_URI_BASE}-pgo-apiserver"}
IMAGE_PGOEVENT=${IMAGE_PGOEVENT:-"${IMAGE_URI_BASE}-pgo-event"}
IMAGE_RMDATA=${IMAGE_RMDATA:-"${IMAGE_URI_BASE}-pgo-rmdata"}
IMAGE_SCHEDULER=${IMAGE_SCHEDULER:-"${IMAGE_URI_BASE}-pgo-scheduler"}
IMAGE_OPERATOR=${IMAGE_OPERATOR:-"${IMAGE_URI_BASE}-postgres-operator"}
IMAGE_DEPLOYER=${IMAGE_DEPLOYER:-"${IMAGE_URI_BASE}-pgo-deployer"}
IMAGE_PGBOUNCER=${IMAGE_PGBOUNCER:-"${IMAGE_BASE}:main-ppg13-pgbouncer"}
IMAGE_PG_HA=${IMAGE_PG_HA:-"${IMAGE_BASE}:main-postgres-ha"}
SKIP_BACKUPS_TO_AWS_GCP=${SKIP_BACKUPS_TO_AWS_GCP:-1}
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)

if [ -f "$conf_dir/cloud-secret.yml" ]; then
	SKIP_BACKUPS_TO_AWS_GCP=''
fi

create_namespace() {
	local namespace="$1"
	local skip_clean_namespace="$2"

	if [[ ${CLEAN_NAMESPACE} == 1 ]] && [[ -z ${skip_clean_namespace} ]]; then
		kubectl_bin get ns \
			| egrep -v "^kube-|^default|Terminating|openshift|^NAME" \
			| awk '{print$1}' \
			| xargs kubectl delete ns &
		kubectl delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
		kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	fi

	if [ -n "${OPENSHIFT}" ]; then
		oc delete project "$namespace" && sleep 40 || :
		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl_bin delete namespace "$namespace" || :
		wait_for_delete "namespace/$namespace"
		kubectl_bin create namespace "$namespace"
		kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
	fi
}

deploy_cert_manager() {
    kubectl_bin create namespace cert-manager || :
    kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
    kubectl_bin apply -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml --validate=false || : 2>/dev/null
    sleep 30
}

desc() {
	set +o xtrace
	local msg="$@"
	printf "\n\n-----------------------------------------------------------------------------------\n"
	printf "$msg"
	printf "\n-----------------------------------------------------------------------------------\n\n"
	set -o xtrace
}

deploy_operator() {
	desc 'start operator'

	# modifing ini structure from configMap
	yq r -d'2' ${src_dir}/deploy/operator.yaml 'data[values.yaml]' \
		| $sed -e "s#namespace: .*#namespace: ${namespace}#g" \
		| $sed -e "s#pgo_operator_namespace: .*#pgo_operator_namespace: ${namespace}#g" \
		| $sed -e "s#pgo_image_tag: .*#pgo_image_tag: ${GIT_BRANCH}#g" \
			>${tmp_dir}/operator.ini

	# updating yaml itself
	yq w -d'*' ${src_dir}/deploy/operator.yaml 'metadata.namespace' ${namespace} \
		| yq w -d'3' - 'subjects[0].namespace' ${namespace} \
		| yq w -d'4' - 'spec.template.spec.containers[0].image' ${IMAGE_DEPLOYER} \
		| yq w -d'2' - -d2 'data[values.yaml]' "$(cat ${tmp_dir}/operator.ini)" \
		| kubectl_bin apply -f -

	wait_job_completion "pgo-deploy"

	wait_pod $(get_operator_pod)
}

deploy_miniogw() {
	local gsc_projectid=${1:-"cloud-dev-112233"}
	kubectl_bin apply -f $conf_dir/cloud-secret-minio-gw.yml \
					  -f $src_dir/deploy/backup/minio-gw-tls.yaml

	yq w -d0 --style=single $src_dir/deploy/backup/minio-gw.yaml 'spec.template.spec.containers[0].args[4]' "${gsc_projectid}" \
		| kubectl_bin apply -f -

	wait_pod "$(kubectl_bin get pod --selector=app=minio-gw -o 'jsonpath={.items[].metadata.name}')"
}

get_operator_pod() {
	kubectl_bin get pods \
		--selector=name=postgres-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

wait_pod_completion() {
	local pod=$1
	local target_phase=${2:-"Succeeded"}

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep "${target_phase}"; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_job_completion() {
	local job=$1

	set +o xtrace
	retry=0
	until kubectl_bin get job/${job} -o jsonpath='{.metadata.name}' 2>/dev/null; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			echo max retry count $retry reached. No target object found.
			exit 1
		fi
	done

	retry=0
	until [[ $(kubectl_bin get job/$job -o jsonpath='{.metadata.name}' 2>&1 | grep -io 'not found') == "not found" ]] \
		|| [[ $(kubectl_bin get job/$job -o jsonpath='{.status.succeeded}' 2>/dev/null) == '1' ]]; do
			sleep 1
			echo -n .
			let retry+=1
			if [ $retry -ge 360 ]; then
				kubectl_bin describe job/$job
				kubectl_bin logs $(kubectl_bin get pods --selector=job-name=${job} --jsonpath='{.items[0].metadata.name}')
				kubectl_bin logs $(get_operator_pod) \
					| grep -v 'level=info' \
					| grep -v 'level=debug' \
					| tail -100
				echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
				exit 1
			fi
	done
	echo
	set -o xtrace
}

apply_cluster() {
	local path=${1}
	local name=${2}
	local backup=${3:-"false"}
	local restore_from=${4:-"false"}
	local restore_opts=${5:-"false"}
	local podAntiAffinity=${6}

	yq w "${path}" 'metadata.namespace' ${namespace} \
		| yq w - 'spec.namespace' ${namespace} \
		| yq w - 'metadata.annotations.current-primary' ${name} \
		| yq w - 'metadata.labels.crunchy-pgha-scope' ${name} \
		| yq w - 'metadata.labels.deployment-name' ${name} \
		| yq w - 'metadata.labels.name' ${name} \
		| yq w - 'metadata.labels.pg-cluster' ${name} \
		| yq w - 'metadata.name' ${name} \
		| yq w - 'spec.PrimaryStorage.name' ${name} \
		| yq w - 'spec.clustername' ${name} \
		| yq w - 'spec.database' ${name} \
		| yq w - 'spec.name' ${name} \
		| yq w - 'spec.user' ${name} \
		| yq w - 'spec.pgImage' ${IMAGE_PG_HA} \
		| yq d - 'spec.pgBouncer' \
			>${tmp_dir}/cr.yml

	case ${podAntiAffinity} in
		'preferred')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		'required')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		'disabled')
			yq w ${tmp_dir}/cr.yml 'spec.podAntiAffinity.default' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBackRest' ${podAntiAffinity} \
				| yq w - 'spec.podAntiAffinity.pgBouncer' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yml
			mv ${tmp_dir}/cr.podAffinity.yml ${tmp_dir}/cr.yml
			;;
		*) ;;
	esac

	if [[ "${backup}" == "true" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.backrestS3Bucket' "operator-testing" \
			| yq w - 'spec.backrestS3Endpoint' 'minio-gateway-svc:9000' \
			| yq w - 'spec.backrestS3Region' 'us-east-1' \
			| yq w - 'spec.backrestS3URIStyle' 'path' \
			| yq w --style=single - 'spec.backrestS3VerifyTLS' 'false' \
			| yq w - 'spec.backrestStorageTypes[0]' 's3' \
				> "${tmp_dir}/cr.backup.yaml"

		# yq w "${tmp_dir}/cr.yaml" 'spec.backrestS3Bucket' "operator-testing" \
		# 	| yq w - 'spec.backrestS3Endpoint' 's3.amazonaws.com' \
		# 	| yq w - 'spec.backrestS3Region' 'us-east-1' \
		# 	| yq w - 'spec.backrestS3URIStyle' 'path' \
		# 	| yq w --style=single - 'spec.backrestS3VerifyTLS' 'true' \
		# 	| yq w - 'spec.backrestStorageTypes[0]' 's3' \
		# 		> "${tmp_dir}/cr.backup.yaml"

		mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
	fi

	if [[ "${restore_from}" != "false" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreFrom' "${restore_from}" \
			> "${tmp_dir}/cr.restore.yaml"
		mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		if [[ "${restore_opts}" != "false" ]]; then
			yq w --style=single "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreOpts' "${restore_opts}" \
				> "${tmp_dir}/cr.restore.yaml"
			mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		fi
	fi

	kubectl_bin apply -f "${tmp_dir}/cr.yaml"
}

kubectl_bin() {
	local LAST_OUT="$(mktemp)"
	local LAST_ERR="$(mktemp)"
	local exit_status=0
	local timeout=4
	for i in $(seq 0 2); do
		kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
		exit_status=$?
		[[ ${-/x/} != $- ]] && echo "--- $i stdout" | cat - "$LAST_OUT" >&$BASH_XTRACEFD
		[[ ${-/x/} != $- ]] && echo "--- $i stderr" | cat - "$LAST_ERR" >&$BASH_XTRACEFD
		if [[ ${exit_status} != 0 ]]; then
			sleep "$((timeout * i))"
		else
			cat "$LAST_OUT"
			cat "$LAST_ERR" >&2
			rm "$LAST_OUT" "$LAST_ERR"
			return ${exit_status}
		fi
	done
	cat "$LAST_OUT"
	cat "$LAST_ERR" >&2
	rm "$LAST_OUT" "$LAST_ERR"
	return ${exit_status}
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_deployment() {
	local name=$1

	sleep 10
	set +o xtrace
	retry=0
	echo -n $name
	until [ "$(kubectl_bin get deployment $name -o jsonpath='{.status.replicas}')" == "$(kubectl_bin get deployment $name -o jsonpath='{.status.readyReplicas}')" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

enable_pgBouncer() {
	local cluster_name=${1}
	local replicas_num=${2:-'1'}

	kubectl_bin patch \
		"pgcluster/${cluster_name}" \
		--type json \
		-p='[{"op":"add","path":"/spec/pgBouncer","value":{"image":"'"${IMAGE_PGBOUNCER}"'","limits":{"cpu":"2","memory":"512Mi"},"replicas":'${replicas_num}',"resources":{"cpu":"1","memory":"128Mi"},"tlsSecret":"","serviceType":"LoadBalancer"}}]'
}

disable_pgbouncer() {
	kubectl_bin patch \
		"pgcluster/${1}" \
		--type json \
		-p='[{"op":"remove","path":"/spec/pgBouncer"}]'
}

compare_kubectl() {
	local resource="$1"
	local postfix="$2"
	local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
	local new_result="${tmp_dir}/${resource//\//_}.yml"

	kubectl_bin get -o yaml ${resource} \
		| yq d - 'metadata.managedFields' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.namespace' \
		| yq d - '**.uid' \
		| yq d - 'metadata.resourceVersion' \
		| yq d - 'metadata.selfLink' \
		| yq d - 'metadata.deletionTimestamp' \
		| yq d - 'metadata.annotations."k8s.v1.cni.cncf.io*"' \
		| yq d - 'metadata.annotations."kubernetes.io/psp"' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.image' \
		| yq d - '**.clusterIP' \
		| yq d - '**.dataSource' \
		| yq d - '**.procMount' \
		| yq d - '**.storageClassName' \
		| yq d - '**.finalizers' \
		| yq d - '**."kubernetes.io/pvc-protection"' \
		| yq d - '**.volumeName' \
		| yq d - '**."volume.beta.kubernetes.io/storage-provisioner"' \
		| yq d - 'spec.volumeMode' \
		| yq d - 'spec.nodeName' \
		| yq d - '**."volume.kubernetes.io/selected-node"' \
		| yq d - '**."percona.com/*"' \
		| yq d - '**.(volumeMode==Filesystem).volumeMode' \
		| yq d - '**.healthCheckNodePort' \
		| yq d - '**.nodePort' \
		| yq d - '**.imagePullSecrets' \
		| yq d - '**.enableServiceLinks' \
		| yq d - 'status' \
		| yq d - '**.(name==suffix)' \
		| yq d - '**.(name==NAMESPACE)' \
		| yq d - 'spec.volumeClaimTemplates.*.apiVersion' \
		| yq d - 'spec.volumeClaimTemplates.*.kind' \
		| yq d - 'metadata.ownerReferences.*.apiVersion' \
		| yq d - '**.controller-uid' \
		| yq d - '**.preemptionPolicy' \
			>${new_result}

	diff -u ${expected_result} ${new_result}
}

spinup_pgcluster() {
	local cluster=$1
	local config=$2
	local backup=${3:-"false"}
	local podAntiAffinity=${4}

	desc 'create first PG cluster'

	yq r -d0 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-pgbouncer-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d1 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-postgres-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d2 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-primaryuser-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -
	yq r -d3 $conf_dir/some-name-secrets.yml \
		| yq w - 'metadata.name' "${cluster}-${cluster}-secret" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| kubectl_bin apply -f -

	desc 'create first PG cluster'
	apply_cluster ${config} ${cluster} ${backup} ${podAntiAffinity}

	wait_job_completion "${cluster}-stanza-create"
	wait_job_completion "backrest-backup-${cluster}"

	wait_cluster_consistency ${cluster}

	desc 'write data'

	run_psql \
		'CREATE DATABASE myapp; \c myapp \\\ CREATE TABLE IF NOT EXISTS myApp (id int PRIMARY KEY);' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ INSERT INTO myApp (id) VALUES (100500)' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	run_psql \
		'\c myapp \\\ GRANT SELECT ON myApp to "some-name";GRANT USAGE ON SCHEMA public TO "some-name";' \
		"postgres:postgres_pass@${cluster}.${namespace}"
	sleep 10
}

wait_cluster_consistency() {
	cluster_name=${1}
	retry=0

	wait_deployment ${1}
	wait_deployment "${1}-backrest-shared-repo"

	until [[ "$(kubectl_bin get Pgcluster "${cluster_name}" -o jsonpath='{.status.state}')" == "pgcluster Initialized" ]]; do
		let retry+=1
		if [ $retry -ge 16 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
		echo 'waiting for cluster readyness'
		sleep 10
	done
}

get_client_pod() {
	kubectl_bin get pods \
		--selector=name=pg-client \
		-o 'jsonpath={.items[].metadata.name}'
}

compare_psql_cmd() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local postfix="$4"
	local uri_suffix="${5}"

	local expected_result=${test_dir}/compare/${command_id}${postfix}.sql

	run_psql "$command" "$uri" "postgres" "$uri_suffix" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql "$command" "$uri" "postgres" "$uri_suffix" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 60 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

run_psql() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://$uri$suffix"
}

destroy() {
	local namespace="$1"

	kubectl_bin logs $(get_operator_pod) \
		| grep -v 'level=info' \
		| grep -v 'level=debug' \
		| grep -v 'Getting tasks for pod' \
		| grep -v 'Getting pods from source' \
		| grep -v 'the object has been modified' \
		| grep -v 'get backup status: Job.batch' \
		| $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
		| sort -u \
		| tee $tmp_dir/operator.log

	kubectl_bin delete -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml 2>/dev/null || :

	if [ -n "$OPENSHIFT" ]; then
		oc delete --grace-period=0 --force=true project "$namespace"
	else
		kubectl_bin delete --grace-period=0 --force=true namespace "$namespace"
	fi
	kubectl_bin delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
	kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	rm -rf ${tmp_dir}
}

deploy_cert_manager() {
	kubectl_bin create namespace cert-manager || :
	kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl_bin apply -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml --validate=false || : 2>/dev/null
	sleep 45
}

create_replica() {
	local cluster=${1}
	local suffix=${2}

	yq w ${conf_dir}/some-name-replica.yml 'metadata.labels.name' "${cluster}-${suffix}" \
		| yq w - 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${cluster}-${suffix}" \
		| yq w - 'spec.clustername' "${cluster}" \
		| yq w - 'spec.name' "${cluster}-${suffix}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.replicastorage.name' "${cluster}-${suffix}" \
		| kubectl_bin apply -f -
}

check_replica() {
	local name=${1}
	local cluster=${2}
	local sql_cmp=${3:-"select-2"}

	run_psql \
		'SELECT usename,application_name,client_addr,state from pg_stat_replication' \
		"postgres:postgres_pass@${cluster}.${namespace}" \
		>${tmp_dir}/replicas.list

	replica_pod_name=$(kubectl_bin get pods --selector=replica-name=${name} -o 'jsonpath={.items[0].metadata.name}')
	if [[ -z "$(grep ${replica_pod_name} ${tmp_dir}/replicas.list | grep "streaming")" ]]; then
		echo "${replica_pod_name} is not connected or has valid data. Exiting..."
		cat "${tmp_dir}/replicas.list"
		exit 1
	fi
	replica_pod_IP=$(kubectl_bin get pods --selector=replica-name=${name} -o 'jsonpath={.items[0].status.podIP}')
	compare_psql_cmd \
		"${sql_cmp}" \
		'\c myapp \\\ SELECT * from myApp;' \
		"some-name:some-name_pass@${replica_pod_IP}" '' ' '
}

create_backup() {
	local cluster=${1}
	local bckp_prefix=${2}
	local bckp_type=${3:-"full"}

	yq w ${test_dir}/conf/backup.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.job-name' "${bckp_prefix}-${cluster}" \
		| yq w --style=single -- - 'spec.parameters.backrest-opts' "--type=${bckp_type}" \
		| yq w - 'spec.parameters.pg-cluster' "${cluster}" \
		| yq w - 'spec.parameters.podname' $(kubectl_bin get pods --selector=name=${cluster}-backrest-shared-repo,pg-cluster=${cluster} -o 'jsonpath={.items[].metadata.name}') \
		| kubectl_bin apply -f -
	sleep 10

	wait_job_completion "${bckp_prefix}-${cluster}"
}

run_restore() {
	local cluster=${1}
	local rstr_prefix=${2}
	local storage=${3:-"local"}

	yq w ${test_dir}/conf/restore.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.backrest-restore-from-cluster' "${cluster}" \
		| yq w - 'spec.parameters.backrest-storage-type' "${storage}" \
		| kubectl_bin apply -f -

	wait_job_completion	"${cluster}-bootstrap"
	wait_job_completion "${cluster}-stanza-create"
	wait_job_completion "backrest-backup-${cluster}"
}
