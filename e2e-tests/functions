#!/bin/bash

GIT_COMMIT=$(git rev-parse HEAD)
GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | sed -e 's/_/-/g' | tr '[:upper:]' '[:lower:]')}
IMAGE_BASE=${IMAGE_BASE:-"perconalab/percona-postgresql-operator"}
IMAGE_URI_BASE=${IMAGE_URI_BASE:-"${IMAGE_BASE}:${GIT_BRANCH}"}
IMAGE_APISERVER=${IMAGE_APISERVER:-"${IMAGE_URI_BASE}-pgo-apiserver"}
IMAGE_PGOEVENT=${IMAGE_PGOEVENT:-"${IMAGE_URI_BASE}-pgo-event"}
IMAGE_RMDATA=${IMAGE_RMDATA:-"${IMAGE_URI_BASE}-pgo-rmdata"}
IMAGE_SCHEDULER=${IMAGE_SCHEDULER:-"${IMAGE_URI_BASE}-pgo-scheduler"}
IMAGE_OPERATOR=${IMAGE_OPERATOR:-"${IMAGE_URI_BASE}-postgres-operator"}
IMAGE_DEPLOYER=${IMAGE_DEPLOYER:-"${IMAGE_URI_BASE}-pgo-deployer"}

PG_VER="${PG_VER:-14}"
IMAGE_PGBOUNCER=${IMAGE_PGBOUNCER:-"${IMAGE_BASE}:main-ppg$PG_VER-pgbouncer"}
IMAGE_PG_HA=${IMAGE_PG_HA:-"${IMAGE_BASE}:main-ppg$PG_VER-postgres-ha"}
IMAGE_BACKREST=${IMAGE_BACKREST:-"${IMAGE_BASE}:main-ppg$PG_VER-pgbackrest"}
IMAGE_BACKREST_REPO=${IMAGE_BACKREST_REPO:-"${IMAGE_BASE}:main-ppg$PG_VER-pgbackrest-repo"}
IMAGE_PGBADGER=${IMAGE_PGBADGER:-"${IMAGE_BASE}:main-ppg$PG_VER-pgbadger"}
IMAGE_PMM=${IMAGE_PMM:-"perconalab/pmm-client:2.29.1"}
PMM_SERVER_VER=${PMM_SERVER_VER:-"9.9.9"}
IMAGE_PMM_SERVER_REPO=${IMAGE_PMM_SERVER_REPO:-"perconalab/pmm-server"}
IMAGE_PMM_SERVER_TAG=${IMAGE_PMM_SERVER_TAG:-"dev-latest"}
SKIP_BACKUPS_TO_AWS_GCP=${SKIP_BACKUPS_TO_AWS_GCP:-1}
BUCKET=${BUCKET:-"pg-operator-testing"}
PGO_K8S_NAME=${PGO_K8S_NAME}
ECR=${ECR:-"119175775298.dkr.ecr.us-east-1.amazonaws.com"}
PLATFORM="GKE"

tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename $test_dir)
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath $test_dir/../conf || :)
src_dir=$(realpath $test_dir/../..)
logs_dir=$(realpath $test_dir/../logs)

if [[ ${ENABLE_LOGGING} == "true" ]]; then
	if [ ! -d "${logs_dir}" ]; then
		mkdir "${logs_dir}"
	fi
	exec &> >(tee ${logs_dir}/${test_name}.log)
	echo "Log: ${logs_dir}/${test_name}.log"
fi

if [ -f "$conf_dir/cloud-secret.yml" ]; then
	SKIP_BACKUPS_TO_AWS_GCP=''
fi

if oc get projects; then
	PLATFORM="OPENSHIFT"
fi

if [ $(kubectl version -o json | jq -r '.serverVersion.gitVersion' | grep "\-eks\-") ]; then
	PLATFORM="EKS"
fi

KUBE_VERSION=$(kubectl version -o json | jq -r '.serverVersion.major + "." + .serverVersion.minor' | $sed -r 's/[^0-9.]+//g')

HELM_VERSION=$(helm version -c | $sed -re 's/.*SemVer:"([^"]+)".*/\1/; s/.*\bVersion:"([^"]+)".*/\1/')
if [ "${HELM_VERSION:0:2}" == "v2" ]; then
	HELM_ARGS="--name"
fi

version_gt() {
	# return true if kubernetes version equal or greater than desired
	if [ $(echo "${KUBE_VERSION} >= $1" | bc -l) -eq 1 ]; then
		return 0
	else
		return 1
	fi
}

cleanup_rbac() {
	kubectl_bin delete clusterrolebindings \
		pgo-cluster-role pgo-deployer-cr \
		chaos-mesh-chaos-controller-manager-cluster-level || true
	kubectl_bin delete clusterroles \
		pgo-cluster-role pgo-deployer-cr \
		chaos-mesh-chaos-controller-manager-target-namespace \
		chaos-mesh-chaos-controller-manager-cluster-level || true
}

destroy_namespace() {
	local namespace=${1:-"$namespace"}

	if [[ ${PLATFORM} == "OPENSHIFT" ]]; then
		oc delete --grace-period=0 --force=true project ${OPERATOR_NS:-$namespace}
	else
		kubectl_bin delete --grace-period=0 --force=true namespace ${OPERATOR_NS:-$namespace}
	fi
}

create_namespace() {
	local namespace="$1"
	local skip_clean_namespace="$2"

	if [[ ${CLEAN_NAMESPACE} == 1 ]] && [[ -z ${skip_clean_namespace} ]]; then
		kubectl_bin get ns \
			| egrep -v "^kube-|^default|Terminating|openshift|^NAME" \
			| awk '{print$1}' \
			| xargs kubectl delete ns &
		cleanup_rbac
		kubectl delete MutatingWebhookConfiguration/chaos-mesh-mutation \
			ValidatingWebhookConfiguration/chaos-mesh-validation \
			ValidatingWebhookConfiguration/validate-auth || true
	fi

	if [[ ${PLATFORM} == "OPENSHIFT" ]]; then
		oc delete project "$namespace" && sleep 40 || :
		oc new-project "$namespace"
		oc project "$namespace"
		oc adm policy add-scc-to-user hostaccess -z default || :
	else
		kubectl_bin delete namespace "$namespace" || :
		wait_for_delete "namespace/$namespace"
		kubectl_bin create namespace "$namespace"
		kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
	fi
}

deploy_cert_manager() {
	kubectl_bin create namespace cert-manager || :
	kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
	kubectl_bin apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.9.0/cert-manager.yaml --validate=false || : 2>/dev/null
	sleep 30
}

desc() {
	set +o xtrace
	local msg="$@"
	printf "\n\n-----------------------------------------------------------------------------------\n"
	printf "$msg"
	printf "\n-----------------------------------------------------------------------------------\n\n"
	set -o xtrace
}

destroy_operator() {
	kubectl_bin ${OPERATOR_NS:+-n $OPERATOR_NS} delete -f ${tmp_dir}/operator.yaml || true
	yq w -d'4' ${tmp_dir}/operator.yaml 'spec.template.spec.containers[0].env[0].value' 'uninstall' \
			>${tmp_dir}/operator-destroy.yaml
	kubectl_bin ${OPERATOR_NS:+-n $OPERATOR_NS} apply -f ${tmp_dir}/operator-destroy.yaml
	wait_job_completion "pgo-deploy" 'true' ${OPERATOR_NS}
	kubectl_bin ${OPERATOR_NS:+-n $OPERATOR_NS} delete -f ${tmp_dir}/operator-destroy.yaml
}

prepare_operator_yaml() {
	local pull_secret_name=${1:-""}
	local operator_manifest=${2:-"${src_dir}/deploy/operator.yaml"}
	local namespace_mode=${3:-"disabled"}
	local operator_action=${4:-"install"}
	local namespace=${5:-"$namespace"}
	local dont_send_telemetry=${6:-"true"}

	yq r -d'2' "${operator_manifest}" 'data[values.yaml]' \
		| $sed -e "s#^namespace: .*#namespace: \"${namespace}\"#g" \
		| $sed -e "s#pgo_operator_namespace: .*#pgo_operator_namespace: \"${OPERATOR_NS:-$namespace}\"#g" \
		| $sed -e "s#namespace_mode: .*#namespace_mode: \"${namespace_mode}\"#g" \
		| $sed -e "s#pgo_image_tag: .*#pgo_image_tag: \"$(echo ${IMAGE_OPERATOR%-postgres-operator} | cut -d: -f2)\"#g" \
		| $sed -e "s#pgo_image_prefix: .*#pgo_image_prefix: \"$(echo ${IMAGE_OPERATOR%-postgres-operator} | cut -d: -f1)\"#g" \
		| $sed -e "s#^disable_telemetry: .*#disable_telemetry: \"${dont_send_telemetry}\"#g" \
		| cat <(echo 'crunchy_debug: "true"') - \
			>${tmp_dir}/operator.ini

	if [[ -n ${pull_secret_name} ]]; then
		$sed -i ${tmp_dir}/operator.ini -e "s#ccp_image_pull_secret: .*#ccp_image_pull_secret: \"${pull_secret_name}\"#g"
		$sed -i ${tmp_dir}/operator.ini -e "s#pgo_image_pull_secret: .*#pgo_image_pull_secret: \"${pull_secret_name}\"#g"
	fi

	if [[ ${PLATFORM} == "OPENSHIFT" ]]; then
		echo 'disable_fsgroup: "true"' >>${tmp_dir}/operator.ini
	fi

	# updating yaml itself
	yq w -d'*' "${operator_manifest}" 'metadata.namespace' ${OPERATOR_NS:-$namespace} \
		| yq w -d'3' - 'subjects[0].namespace' ${OPERATOR_NS:-$namespace} \
		| yq w -d'4' - 'spec.template.spec.containers[0].image' ${IMAGE_DEPLOYER} \
		| yq w -d'2' - -d2 'data[values.yaml]' "$(cat ${tmp_dir}/operator.ini)" \
		| yq w -d'4' - 'spec.template.spec.containers[0].env[0].value' "${operator_action}" \
			>${tmp_dir}/operator.yaml

	if [[ -n ${pull_secret_name} ]]; then
		yq w -i -d'0' "${tmp_dir}/operator.yaml" 'imagePullSecrets[0].name' "${pull_secret_name}"
		yq w -i -d'4' "${tmp_dir}/operator.yaml" 'spec.template.spec.imagePullSecrets[0].name' "${pull_secret_name}"
	fi
}

deploy_operator() {
	local pull_secret_name=${1:-""}
	local operator_manifest=${2:-"${src_dir}/deploy/operator.yaml"}
	local namespace_mode=${3:-"disabled"}
	local action=${4:-"install"}
	local namespace=${5:-"${namespace}"}
	local dont_send_telemetry=${6:-"false"}
	local version_service_uri_override=${7}

	desc 'start operator'

	prepare_operator_yaml "${pull_secret_name}" "${operator_manifest}" "${namespace_mode}" "${action}" "${namespace}" "${dont_send_telemetry}"
	kubectl_bin apply ${OPERATOR_NS:+-n $OPERATOR_NS} -f ${tmp_dir}/operator.yaml

	wait_job_completion "pgo-deploy" 'true' ${OPERATOR_NS}
	kubectl_bin delete ${OPERATOR_NS:+-n $OPERATOR_NS} -f ${tmp_dir}/operator.yaml

	wait_pod $(get_operator_pod) ${OPERATOR_NS}
	if [[ -n ${version_service_uri_override} ]]; then
		kubectl_bin -n ${namespace} patch deployment/postgres-operator --type json -p='[{"op":"add","path":"/spec/template/spec/containers/1/env/-","value":{"name":"PERCONA_VS_FALLBACK_URI","value":"'${version_service_uri_override}'"}}]'
		sleep 15
	fi
}

retry() {
	local max=$1
	local delay=$2
	shift 2 # cut delay and max args
	local n=1

	until "$@"; do
		if [[ $n -ge $max ]]; then
			echo "The command '$@' has failed after $n attempts."
			exit 1
		fi
		((n++))
		sleep $delay
	done
}

deploy_minio() {
	desc 'install Minio'
	kubectl_bin apply -f $src_dir/deploy/backup/minio-gw-tls.yaml
	helm uninstall minio-service || :
	helm repo remove minio || :
	helm repo add minio https://helm.min.io/
	# kubectl_bin delete pvc minio-service --force
	retry 10 60 helm install minio-service \
		--version 8.0.5 \
		--set tls.enabled=true \
		--set accessKey=some-access-key \
		--set secretKey=some-secret-key \
		--set service.type=ClusterIP \
		--set configPathmc=/tmp/.minio/ \
		--set persistence.size=2G \
		--set environment.MINIO_REGION=us-east-1 \
		--set environment.MINIO_HTTP_TRACE=/tmp/trace.log \
		--set securityContext.enabled=false \
		--set tls.certSecret=minio-gw-tls \
		--set tls.publicCrt=tls.crt \
		--set tls.privateKey=tls.key \
		minio/minio
	MINIO_POD=$(kubectl_bin get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
	wait_pod $MINIO_POD
	kubectl_bin run -i aws-cli --rm --image=perconalab/awscli --restart=Never \
		--overrides='{"spec":{"containers":[{"args":["/usr/bin/env","AWS_ACCESS_KEY_ID=some-access-key","AWS_SECRET_ACCESS_KEY=some-secret-key","AWS_DEFAULT_REGION=us-east-1","AWS_CA_BUNDLE=/tmp/CAs/ca.crt","/usr/bin/aws","--endpoint-url","https://minio-service:9000","s3","mb","s3://pg-operator-testing"],"imagePullPolicy":"Always","image":"perconalab/awscli","name":"awscli","volumeMounts":[{"mountPath":"/tmp/CAs","name":"minio-ca"}]}],"volumes":[{"name":"minio-ca","secret":{"defaultMode":420,"items":[{"key":"ca.crt","path":"ca.crt"}],"secretName":"minio-gw-tls"}}]}}'
}

destroy_minio() {
	desc 'destroy Minio'
	helm uninstall minio-service
}

get_operator_pod() {
	kubectl_bin get pods \
		--selector=name=postgres-operator \
		-o 'jsonpath={.items[].metadata.name}'
}

wait_pod_completion() {
	local pod=$1
	local target_phase=${2:-"Succeeded"}

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep "${target_phase}"; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin describe pod/$pod
			kubectl_bin logs $pod
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

calculate_next_backup_start() {
	local start_time_sec=$1
	local backup_time_interval_sec=$2
	local next_interval_hop=$((start_time_sec + backup_time_interval_sec))
	local next_interval_min_edge=$((next_interval_hop % backup_time_interval_sec))
	local next_backup_at_sec=$((next_interval_hop - next_interval_min_edge))
	echo ${next_backup_at_sec}
}

get_current_epoch_time() {
	local curent_date=$(curl -sI https://percona.com/ | grep date | $sed 's/date: //g')
	echo $($date --date "${curent_date}" --utc +%s)
}

next_backup_after_sec() {
	# configmap is to contain schedule like '*/N * * * *'
	local conf_map_name=$1

	local configmap_creation_time=$($date '+%s' --date=$(kubectl_bin get configmap/${conf_map_name} -o jsonpath='{.metadata.creationTimestamp}') --utc)
	local backup_time_interval_sec=$(($(kubectl_bin get configmap/${conf_map_name} -o jsonpath='{.data.schedule}' | jq '.schedule' | grep -Eo '[0-9]+') * 60))

	local result=$((\
		$(calculate_next_backup_start ${configmap_creation_time} ${backup_time_interval_sec}) - \
		$(get_current_epoch_time)))
	until [[ ${result} -gt 0 ]]; do
		result=$((\
			$(calculate_next_backup_start $(get_current_epoch_time) ${backup_time_interval_sec}) - \
			$(get_current_epoch_time)))
	done
	echo ${result}
}

wait_pod() {
	local pod=$1
	local ns=$2

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl_bin ${ns:+-n $ns} get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin ${ns:+-n $ns} describe pod/$pod
			kubectl_bin ${ns:+-n $ns} logs $pod
			kubectl_bin ${ns:+-n $ns} logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_job_completion() {
	local job=$1
	local mandatory=${2:-"true"}
	local ns=${3:-"$namespace"}

	retry=0
	set +o xtrace
	until KUBECONFIG=${TARGET_CONFIG:-"${KUBECONFIG}"} kubectl -n $ns get job/${job} -o jsonpath='{.metadata.name}' 2>/dev/null; do
		sleep 1
		echo -n .
		let retry+=1
		if [[ $retry -ge 90 && ${mandatory} == "true" ]]; then
			echo max retry count $retry reached. No target object found.
			exit 1
		elif [[ $retry -ge 90 && ${mandatory} != "true" ]]; then
			echo Can not detect job. Passing by.
			set -o xtrace
			return 0
		fi
	done

	retry=0
	until [[ $(kubectl_bin -n $ns get job/$job -o jsonpath='{.metadata.name}' 2>&1 | grep -io 'not found') == "not found" ]] \
		|| [[ $(kubectl_bin -n $ns get job/$job -o jsonpath='{.status.succeeded}' 2>/dev/null) == '1' ]]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 600 ]; then
			kubectl_bin describe -n $ns job/$job
			kubectl_bin logs $(kubectl_bin -n $ns get pods --selector=job-name=${job} -o jsonpath='{.items[0].metadata.name}') -n $ns
			kubectl_bin -n $ns logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

apply_cluster() {
	local path=${1}
	local name=${2}
	local backup=${3:-"false"}
	local restore_from=${4:-"false"}
	local restore_opts=${5:-"false"}
	local custom_config=${6:-"false"}
	local podAntiAffinity=${7}
	local schedule=${8}
	local schedule_type=${9}
	local tls_enabled=${10}
	local root_via_pgbouncer=${11}

	yq w "${path}" 'metadata.annotations.current-primary' ${name} \
		| yq w - 'metadata.labels.crunchy-pgha-scope' ${name} \
		| yq w - 'metadata.labels.deployment-name' ${name} \
		| yq w - 'metadata.labels.name' ${name} \
		| yq w - 'metadata.labels.pg-cluster' ${name} \
		| yq w - 'metadata.name' ${name} \
		| yq w - 'spec.clustername' ${name} \
		| yq w - 'spec.database' ${name} \
		| yq w - 'spec.name' ${name} \
		| yq w - 'spec.keepData' false \
		| yq w - 'spec.keepBackups' false \
		| yq w - 'spec.user' some-name \
		| yq w - 'spec.secretsName' "${name}-users-secret" \
		| yq w - 'spec.pgPrimary.image' ${IMAGE_PG_HA} \
		| yq w - 'spec.backup.image' ${IMAGE_BACKREST} \
		| yq w - 'spec.backup.backrestRepoImage' ${IMAGE_BACKREST_REPO} \
		| yq w - 'spec.pgBadger.image' ${IMAGE_PGBADGER} \
		| yq w - 'spec.pmm.image' ${IMAGE_PMM} \
		| yq w - 'spec.pgBouncer.image' ${IMAGE_PGBOUNCER} \
			>"${tmp_dir}/cr.yaml"


	case ${podAntiAffinity} in
		'preferred')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.affinity.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		'required')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.affinity.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		'disabled')
			yq w ${tmp_dir}/cr.yaml 'spec.pgPrimary.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.backup.affinity.antiAffinityType' ${podAntiAffinity} \
				| yq w - 'spec.pgBouncer.affinity.antiAffinityType' ${podAntiAffinity} \
					>${tmp_dir}/cr.podAffinity.yaml
			mv ${tmp_dir}/cr.podAffinity.yaml ${tmp_dir}/cr.yaml
			;;
		*) ;;
	esac

	case ${backup} in
		'gcs')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'gcs+'*)
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
				| yq w - 'spec.backup.repoPath' "/backrestrepo/${backup//'gcs+'/}-backrest-shared-repo" \
				| yq w - 'spec.standby' true \
				| yq d - 'spec.pgBouncer' \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'minio')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 'minio-service:9000' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
				| yq w - 'spec.backup.storages[my-s3].verifyTLS' false \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		's3')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
					>"${tmp_dir}/cr.backup.yaml"
			if [[ ${schedule} == "true" ]]; then
				yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
				| yq w - 'spec.backup.schedule[0].name' 'sch-backup' \
				| yq w - 'spec.backup.schedule[0].schedule' '*/5 * * * *' \
				| yq w - 'spec.backup.schedule[0].type' ${schedule_type} \
				| yq w - 'spec.backup.schedule[0].storage' 'my-s3' \
					>"${tmp_dir}/cr.backup.yaml"
			fi
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'local,s3')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
					>"${tmp_dir}/cr.backup.yaml"
			if [[ ${schedule} == "true" ]]; then
				yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-s3].bucket' ${BUCKET} \
				| yq w - 'spec.backup.storages[my-s3].type' 's3' \
				| yq w - 'spec.backup.storages[my-s3].endpointUrl' 's3.amazonaws.com' \
				| yq w - 'spec.backup.storages[my-s3].region' 'us-east-1' \
				| yq w - 'spec.backup.storages[my-s3].uriStyle' 'path' \
				| yq w - 'spec.backup.schedule[0].name' 'sch-backup' \
				| yq w - 'spec.backup.schedule[0].schedule' '*/5 * * * *' \
				| yq w - 'spec.backup.schedule[0].type' ${schedule_type} \
				| yq w - 'spec.backup.schedule[0].storage' 'my-s3' \
					>"${tmp_dir}/cr.backup.yaml"
			fi
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		'local,gcs')
			yq w "${tmp_dir}/cr.yaml" 'spec.backup.storages[my-gcs].type' 'gcs' \
				| yq w - 'spec.backup.storages[my-gcs].bucket' ${BUCKET} \
					>"${tmp_dir}/cr.backup.yaml"
			mv "${tmp_dir}/cr.backup.yaml" "${tmp_dir}/cr.yaml"
			;;
		*) ;;
	esac

	if [[ ${restore_from} != "false" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreFrom' "${restore_from}" \
			>"${tmp_dir}/cr.restore.yaml"
		mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		if [[ ${restore_opts} != "false" ]]; then
			yq w --style=single "${tmp_dir}/cr.yaml" 'spec.pgDataSource.restoreOpts' "${restore_opts}" \
				>"${tmp_dir}/cr.restore.yaml"
			mv "${tmp_dir}/cr.restore.yaml" "${tmp_dir}/cr.yaml"
		fi
	fi

	if [[ ${custom_config} != "false" ]]; then
		yq w -i "${tmp_dir}/cr.yaml" 'spec.pgPrimary.customconfig' ${custom_config}
	fi

	if [[ ${tls_enabled} == "true" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.tlsOnly' true \
			| yq w - 'spec.sslCA' 'cluster1-ssl-ca' \
			| yq w - 'spec.sslSecretName' 'cluster1-ssl-keypair' \
			| yq w - 'spec.sslReplicationSecretName' 'cluster1-ssl-keypair' \
				>"${tmp_dir}/cr.tls.yaml"
		mv "${tmp_dir}/cr.tls.yaml" "${tmp_dir}/cr.yaml"
	fi

	if [[ ${root_via_pgbouncer} == "true" ]]; then
		yq w "${tmp_dir}/cr.yaml" 'spec.pgBouncer.exposePostgresUser' true \
				>"${tmp_dir}/cr.root_pgb.yaml"
		mv "${tmp_dir}/cr.root_pgb.yaml" "${tmp_dir}/cr.yaml"
	fi

	kubectl_bin apply -f "${tmp_dir}/cr.yaml" -n $namespace
}

kubectl_bin() {
	local LAST_OUT="$(mktemp)"
	local LAST_ERR="$(mktemp)"
	local exit_status=0
	local timeout=4
	set +o errexit
	for i in $(seq 0 2); do
		KUBECONFIG=${TARGET_CONFIG:-"${KUBECONFIG}"} kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
		exit_status=$?
		if [[ ${exit_status} != 0 ]]; then
			sleep "$((timeout * i))"
		else
			break
		fi
	done
	set -o errexit
	cat "$LAST_OUT"
	cat "$LAST_ERR" >&2
	rm "$LAST_OUT" "$LAST_ERR"
	return ${exit_status}
}

wait_for_delete() {
	local res="$1"

	set +o xtrace
	echo -n "$res - "
	retry=0
	until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 120 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

wait_deployment() {
	local name=$1
	local target_namespace=${2:-"$namespace"}

	sleep 10
	set +o xtrace
	retry=0
	echo -n $name
	until [ -n "$(kubectl_bin -n ${target_namespace} get deployment $name -o jsonpath='{.status.replicas}')" \
		-a "$(kubectl_bin -n ${target_namespace} get deployment $name -o jsonpath='{.status.replicas}')" \
		== "$(kubectl_bin -n ${target_namespace} get deployment $name -o jsonpath='{.status.readyReplicas}')" ]; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	echo
	set -o xtrace
}

wait_generation_uniformity() {
	local cluster=$1
	local target_revision=$2

	set +o xtrace
	retry=0
	uniq_generation_values=0
	echo -n $name
	until [[ ${uniq_generation_values} == 1 ]]; do
		sleep 1
		uniq_generation_values=$(kubectl_bin get deployments \
			--selector=pg-cluster=${cluster} -o jsonpath='{range .items[*]}{.metadata.annotations.deployment\.kubernetes\.io\/revision}{"\n"}{end}' \
			| sort -s -u | wc -l | grep -Eo '[0-9]+')
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl_bin logs $(get_operator_pod) \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	if [[ $(kubectl_bin get deployments --selector=pg-cluster=${cluster} -o jsonpath='{range .items[*]}{.metadata.annotations.deployment\.kubernetes\.io\/revision}{"\n"}{end}' \
		| uniq) != ${target_revision} ]]; then
		echo "Unexpected deployment generation revision. Exiting..."
		exit 1
	fi
	echo
	set -o xtrace
}

wait_smart_update() {
	local cluster=$1
	local target=$2

	set +o xtrace
	retry=0

	while [[ $(kubectl_bin logs $(get_operator_pod) operator | grep -c "Smart update finished") == ${target} ]]; do
		sleep 1

		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	sleep 90
	echo
	set -o xtrace
}

enable_pgBouncer() {
	local cluster_name=${1}
	local replicas_num=${2:-'1'}

	kubectl_bin patch \
		"perconapgcluster/${cluster_name}" \
		--type json \
		-p='[{"op":"replace","path":"/spec/pgBouncer/size","value":'${replicas_num}'}]'
	sleep 60
	kubectl_bin wait --timeout=300s --for=condition=Available "deployment/${cluster}-pgbouncer"
}

disable_pgBouncer() {
	kubectl_bin patch \
		"perconapgcluster/${1}" \
		--type json \
		-p='[{"op":"replace","path":"/spec/pgBouncer/size","value":0}]'
	kubectl_bin wait --timeout=300s --for=delete "deployment/${1}-pgbouncer"
}

compare_kubectl() {
	local resource="$1"
	local postfix="$2"
	local expected_result=${test_dir}/compare/${resource//\//_}${postfix}.yml
	local new_result="${tmp_dir}/${resource//\//_}.yml"

	if [[ ${PLATFORM} == "OPENSHIFT" && -f ${expected_result//.yml/-oc.yml} ]]; then
		expected_result=${expected_result//.yml/-oc.yml}
	fi

	kubectl_bin get -o yaml ${resource} \
		| yq d - 'metadata.managedFields' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.namespace' \
		| yq d - '**.uid' \
		| yq d - 'metadata.resourceVersion' \
		| yq d - 'metadata.selfLink' \
		| yq d - 'metadata.deletionTimestamp' \
		| yq d - 'metadata.annotations."k8s.v1.cni.cncf.io*"' \
		| yq d - 'metadata.annotations."kubernetes.io/psp"' \
		| yq d - 'metadata.annotations."cloud.google.com/neg"' \
		| yq d - '**.creationTimestamp' \
		| yq d - '**.image' \
		| yq d - '**.clusterIP' \
		| yq d - '**.clusterIPs' \
		| yq d - '**.dataSource' \
		| yq d - '**.procMount' \
		| yq d - '**.storageClassName' \
		| yq d - '**.finalizers' \
		| yq d - '**."kubernetes.io/pvc-protection"' \
		| yq d - '**.volumeName' \
		| yq d - '**."volume.beta.kubernetes.io/storage-provisioner"' \
		| yq d - '**."volume.kubernetes.io/storage-provisioner"' \
		| yq d - 'spec.volumeMode' \
		| yq d - 'spec.nodeName' \
		| yq d - '**."volume.kubernetes.io/selected-node"' \
		| yq d - '**."percona.com/*"' \
		| yq d - '**.(volumeMode==Filesystem).volumeMode' \
		| yq d - '**.healthCheckNodePort' \
		| yq d - '**.nodePort' \
		| yq d - '**.imagePullSecrets' \
		| yq d - '**.enableServiceLinks' \
		| yq d - 'status' \
		| yq d - '**.(name==suffix)' \
		| yq d - '**.(name==NAMESPACE)' \
		| yq d - 'spec.volumeClaimTemplates.*.apiVersion' \
		| yq d - 'spec.volumeClaimTemplates.*.kind' \
		| yq d - 'metadata.ownerReferences.*.apiVersion' \
		| yq d - '**.controller-uid' \
		| yq d - '**.preemptionPolicy' \
		| yq d - '**.ipFamilies' \
		| yq d - '**.ipFamilyPolicy' \
		| yq d - '**.internalTrafficPolicy' \
		| yq d - '**.allocateLoadBalancerNodePorts' \
			>${new_result}

	if [[ $(yq r ${new_result} 'kind') == 'Service' ]] \
		&& [[ "x$(yq r ${new_result} 'metadata.annotations')" == 'x' || "x$(yq r ${new_result} 'metadata.annotations')" == 'x{}' ]]; then
		yq d -i ${new_result} 'metadata.annotations'
	fi

	diff -u ${expected_result} ${new_result}
}

spinup_pgcluster() {
	local cluster=$1
	local config=$2
	local backup=${3:-"false"}
	local custom_config=${4:-"false"}
	local podAntiAffinity=${5:-"false"}
	local schedule=${6:-"false"}
	local schedule_type=${7:-"false"}
	local tls_enabled=${8:-"false"}
	local pgoClientFile="${9:-$conf_dir/client.yml}"
	local namespace="${10:-"$namespace"}"
	local root_via_pgbouncer="${11}"

	desc 'create fresh PG cluster'
	create_user_secrets ${cluster} ${namespace}
	kubectl_bin -n $namespace apply -f ${pgoClientFile}
	apply_cluster ${config} ${cluster} ${backup} 'false' 'false' ${custom_config} ${podAntiAffinity} ${schedule} ${schedule_type} ${tls_enabled} ${root_via_pgbouncer}
	wait_deployment "${cluster}-backrest-shared-repo"
	wait_deployment "${cluster}"
	# Exiting since we are running cluster in stanby mode
	[[ ${backup} == 'gcs+'* ]] && return

	wait_job_completion "${cluster}-stanza-create" 'false'
	wait_job_completion "backrest-backup-${cluster}"

	wait_cluster_consistency ${cluster}
	sleep 10
	desc 'write data'

	run_psql \
		'CREATE DATABASE myapp; \c myapp \\\ CREATE TABLE IF NOT EXISTS myApp (id int PRIMARY KEY);' \
		"postgres:$(get_psql_user_pass postgres ${cluster})@${cluster}.$namespace"
	run_psql \
		'\c myapp \\\ INSERT INTO myApp (id) VALUES (100500)' \
		"postgres:$(get_psql_user_pass postgres ${cluster})@${cluster}.$namespace"
	run_psql \
		'\c myapp \\\ GRANT SELECT,INSERT ON myApp to "some-name";GRANT USAGE ON SCHEMA public TO "some-name";' \
		"postgres:$(get_psql_user_pass postgres ${cluster})@${cluster}.$namespace"
	sleep 10
}

create_user_secrets() {
	local cluster=$1
	local namespace="${2:-"$namespace"}"
	yq r -d0 $src_dir/deploy/users-secret.yaml \
		| yq w - 'metadata.name' "${cluster}-users-secret" \
		| yq w - 'stringData.some-name' "some-name_pass" \
		| kubectl_bin -n ${namespace} apply -f -
	yq r -d1 $src_dir/deploy/users-secret.yaml \
		| yq w - 'metadata.name' "${cluster}-pmm-secret" \
		| kubectl_bin -n ${namespace} apply -f -
}

get_psql_user_pass() {
	local user=${1}
	local cluster=${2:-"some-name"}
	local namespace="${3:-"$namespace"}"

	if [[ "x$(kubectl_bin -n ${namespace} get perconapgcluster/${cluster} -o jsonpath='{.metadata.name}')" == "x" ]]; then
		: no cluster ${cluster} is running.
		return 0
	fi

	case $(kubectl_bin get perconapgcluster/${cluster_name} -o jsonpath='{.metadata.labels.pgo-version}') in
		'1.0.0' | '0.2.0')
			echo "$(kubectl_bin -n ${namespace} get secret/${cluster_name}-${user}-secret -o jsonpath='{.data.password}' | base64 -d)"
			;;
		*)
			if [[ "x$(kubectl_bin -n ${namespace} get perconapgcluster/${cluster} -o jsonpath='{.spec.secretsName}')" == "x" ]]; then
				: assuming user secret was created by default
				echo $(kubectl_bin -n ${namespace} get "secret/${cluster}-users-secret" -o jsonpath='{.data.'${user}'}' | base64 -d)
			else
				echo $(kubectl_bin -n ${namespace} get "secret/$(kubectl_bin -n ${namespace} get perconapgcluster/${cluster} -o jsonpath='{.spec.secretsName}')" -o jsonpath='{.data.'${user}'}' | base64 -d)
			fi
			;;
	esac
}

wait_cluster_consistency() {
	cluster_name=${1}

	wait_cluster_status ${cluster_name} "pgcluster Initialized"
}

wait_cluster_status() {
	local cluster_name=${1}
	local target_status=${2}
	local target_namespace=${3:-"$namespace"}
	local status_query=''

	case $(kubectl_bin get perconapgcluster/${cluster_name} -o jsonpath='{.metadata.labels.pgo-version}') in
		'1.1.0' | '1.0.0' | '0.2.0')
			status_query="kubectl_bin -n ${target_namespace} get pgcluster/${cluster_name} -o jsonpath='{.status.state}'"
			;;
		*)
			status_query="kubectl_bin -n ${target_namespace} get perconapgcluster/${cluster_name} -o jsonpath='{.status.PGCluster.state}'"
			;;
	esac
	local retry=0
	until [[ "$(eval ${status_query})" == "${target_status}" ]]; do
		let retry+=1
		if [ $retry -ge 24 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
		echo 'waiting for cluster readyness'
		sleep 10
	done
}

get_client_pod() {
	kubectl_bin get pods \
		--selector=name=pg-client \
		-o 'jsonpath={.items[].metadata.name}'
}

compare_psql_cmd() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local postfix="$4"
	local uri_suffix="${5}"

	local expected_result=${test_dir}/compare/${command_id}${postfix}.sql

	run_psql "$command" "$uri" "postgres" "$uri_suffix" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql "$command" "$uri" "postgres" "$uri_suffix" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

compare_psql() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local sslmode="$4"

	local expected_result=${test_dir}/compare/${command_id}.sql

	run_psql_ssl "$command" "$uri" "postgres" "" "$sslmode" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql_ssl "$command" "$uri" "postgres" "" "$sslmode" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

run_psql_ssl() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin -n $namespace get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')
	local sslmode="$5"

	kubectl_bin exec ${client_container} -n $namespace -- \
		bash -c "printf '$command\n' | PGSSLMODE=$sslmode PGSSLROOTCERT=/tmp/tls/ca.crt psql -v ON_ERROR_STOP=1 -t -q $driver://$uri$suffix" || true
}

run_psql() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}
	local suffix=${4:-.svc.cluster.local}
	local client_container=$(kubectl_bin -n $namespace get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl_bin exec ${client_container} -n $namespace -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://$uri$suffix"
}

destroy() {
	kubectl_bin ${OPERATOR_NS:+-n $OPERATOR_NS} logs $(get_operator_pod) \
		| grep -v 'level=info' \
		| grep -v 'level=debug' \
		| grep -v 'Getting tasks for pod' \
		| grep -v 'Getting pods from source' \
		| grep -v 'the object has been modified' \
		| grep -v 'get backup status: Job.batch' \
		| $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
		| sort -u \
		| tee $tmp_dir/operator.log

	destroy_operator

	kubectl_bin get ns \
		| egrep -v "^kube-|^default|Terminating|openshift|^NAME" \
		| awk '{print$1}' \
		| xargs kubectl delete ns --grace-period=0 --force=true

	kubectl_bin delete clusterrolebindings pgo-cluster-role pgo-deployer-cr || true
	kubectl_bin delete clusterroles pgo-cluster-role pgo-deployer-cr || true
	rm -rf ${tmp_dir}
}

check_replica() {
	local name=${1}
	local cluster=${2}
	local sql_cmp=${3:-"select-2"}

	run_psql \
		'SELECT usename,application_name,client_addr,state from pg_stat_replication' \
		"postgres:$(get_psql_user_pass postgres ${cluster})@${cluster}.${namespace}" \
		>${tmp_dir}/replicas.list

	replica_pod_name=$(kubectl_bin -n $namespace get pods --selector=deployment-name=${name},name=${cluster}-replica -o 'jsonpath={.items[0].metadata.name}')
	if [[ -z "$(grep ${replica_pod_name} ${tmp_dir}/replicas.list | grep "streaming")" ]]; then
		echo "${replica_pod_name} is not connected or has valid data. Exiting..."
		cat "${tmp_dir}/replicas.list"
		exit 1
	fi
	replica_pod_IP=$(kubectl_bin -n $namespace get pods --selector=deployment-name=${name},name=${cluster}-replica -o 'jsonpath={.items[0].status.podIP}')
	compare_psql_cmd \
		"${sql_cmp}" \
		'\c myapp \\\ SELECT * from myApp;' \
		"some-name:some-name_pass@${replica_pod_IP}" '' ' '
}

create_backup() {
	local cluster=${1}
	local bckp_prefix=${2}
	local bckp_type=${3:-"full"}

	yq w ${conf_dir}/backup.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.name' "${bckp_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.job-name' "${bckp_prefix}-${cluster}" \
		| yq w --style=single -- - 'spec.parameters.backrest-opts' "--type=${bckp_type}" \
		| yq w - 'spec.parameters.pg-cluster' "${cluster}" \
		| yq w - 'spec.parameters.podname' $(kubectl_bin get pods --selector=name=${cluster}-backrest-shared-repo,pg-cluster=${cluster} -o 'jsonpath={.items[].metadata.name}') \
		| kubectl_bin apply -f -
	sleep 10

	wait_job_completion "${bckp_prefix}-${cluster}"
}

run_restore() {
	local cluster=${1}
	local rstr_prefix=${2}
	local storage=${3:-"posix"}
	local target=${4:-"null"} #time, xid or name
	local type=${5:-"time"}

	yq w ${test_dir}/conf/restore.yml 'metadata.labels.pg-cluster' "${cluster}" \
		| yq w - 'metadata.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.name' "${rstr_prefix}-${cluster}" \
		| yq w - 'spec.namespace' "${namespace}" \
		| yq w - 'spec.parameters.backrest-restore-cluster' "${cluster}" \
		| yq w - 'spec.parameters.backrest-storage-type' "${storage}" \
			>${tmp_dir}/restore.yml

	if [[ ${target} != "null" ]]; then
		yq w ${tmp_dir}/restore.yml --style=single 'spec.parameters.backrest-pitr-target' "${target}" \
			| yq w --style=single -- - 'spec.parameters.backrest-restore-opts' "--type=${type}" \
				>${tmp_dir}/restore.pitr.yml
		mv ${tmp_dir}/restore.pitr.yml ${tmp_dir}/restore.yml
	fi

	kubectl apply -f ${tmp_dir}/restore.yml

	wait_bootstrap_completeness ${cluster}
}

wait_bootstrap_completeness() {
	local cluster=$1

	wait_job_completion "${cluster}-bootstrap"
	wait_deployment "${cluster}-backrest-shared-repo"
	wait_deployment "${cluster}"
	wait_job_completion "${cluster}-stanza-create" 'false'
	wait_job_completion "backrest-backup-${cluster}"
}

get_service_endpoint() {
	local service=$1

	local hostname=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].hostname' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$hostname" -a "$hostname" != "null" ]; then
		echo $hostname
		return
	fi

	local ip=$(
		kubectl_bin get service/$service -o json \
			| jq '.status.loadBalancer.ingress[].ip' \
			| sed -e 's/^"//; s/"$//;'
	)
	if [ -n "$ip" -a "$ip" != "null" ]; then
		echo $ip
		return
	fi

	exit 1
}

deploy_helm() {
	helm repo add hashicorp https://helm.releases.hashicorp.com
	helm repo add percona https://percona-charts.storage.googleapis.com/
	helm repo update
}

deploy_chaos_mesh() {
	local chaos_mesh_ns=$1

	desc 'install chaos-mesh'
	local old_cm_namespace=$(helm list --all-namespaces --filter chaos-mesh | tail -n1 | awk -F' ' '{print $2}')
	if [ "${old_cm_namespace}" != "NAMESPACE" ]; then
		helm del chaos-mesh --namespace ${old_cm_namespace} || :
	fi
	helm repo add chaos-mesh https://charts.chaos-mesh.org
	case ${PLATFORM} in
		"OPENSHIFT")
			oc adm policy add-scc-to-user privileged -n ${chaos_mesh_ns} -z chaos-daemon
			helm install chaos-mesh chaos-mesh/chaos-mesh \
				--namespace=${chaos_mesh_ns} \
				--set chaosDaemon.runtime=crio \
				--set chaosDaemon.socketPath=/var/run/crio/crio.sock \
				--set dashboard.create=false \
				--version v2.0.4 \
				--set clusterScoped=false \
				--set controllerManager.targetNamespace=${chaos_mesh_ns}
			;;
		"EKS")
			helm install chaos-mesh chaos-mesh/chaos-mesh \
				--namespace=${chaos_mesh_ns} \
				--set chaosDaemon.runtime=containerd \
				--set chaosDaemon.socketPath=/var/run/dockershim.sock \
				--set dashboard.create=false \
				--version v2.0.4 \
				--set clusterScoped=false \
				--set controllerManager.targetNamespace=${chaos_mesh_ns}
			;;
		*)
			if version_gt "1.19"; then
				helm install chaos-mesh chaos-mesh/chaos-mesh \
					--namespace=${chaos_mesh_ns} \
					--set chaosDaemon.runtime=containerd \
					--set chaosDaemon.socketPath=/run/containerd/containerd.sock \
					--set dashboard.create=false \
					--version v2.0.4 \
					--set clusterScoped=false \
					--set controllerManager.targetNamespace=${chaos_mesh_ns}
			else
				helm install chaos-mesh chaos-mesh/chaos-mesh \
					--namespace=${chaos_mesh_ns} \
					--set dashboard.create=false \
					--version v2.0.4 \
					--set clusterScoped=false \
					--set controllerManager.targetNamespace=${chaos_mesh_ns}
			fi
			;;
	esac

	sleep 10
}

destroy_chaos_mesh() {
	local chaos_mesh_ns=$1

	desc 'destroy chaos-mesh'
	helm del chaos-mesh --namespace ${chaos_mesh_ns} || :
}

kill_pod() {
	local pod=$1

	cat $conf_dir/chaos-pod-kill.yml \
		| yq w - "metadata.name" "chaos-cluster-pod-kill-$RANDOM" \
		| yq w - "metadata.namespace" "$namespace" \
		| yq w - "spec.selector.pods.$namespace[0]" "$pod" \
		| kubectl_bin apply -f -
	sleep 5
}

failure_pod() {
	local pod=$1

	cat $conf_dir/chaos-pod-failure.yml \
		| yq w - "metadata.name" "chaos-cluster-pod-failure-$RANDOM" \
		| yq w - "metadata.namespace" "$namespace" \
		| yq w - "spec.selector.pods.$namespace[0]" "$pod" \
		| kubectl_bin apply -f -
	sleep 10
}

network_loss() {
	local pod=$1

	cat $conf_dir/chaos-network-loss.yml \
		| yq w - "metadata.name" "chaos-cluster-network-loss-$RANDOM" \
		| yq w - "metadata.namespace" "$namespace" \
		| yq w - "spec.selector.pods.$namespace[0]" "$pod" \
		| kubectl_bin apply -f -
}

compare_psql() {
	local command_id="$1"
	local command="$2"
	local uri="$3"
	local sslmode="$4"

	local expected_result=${test_dir}/compare/${command_id}.sql

	run_psql_ssl "$command" "$uri" "postgres" "" "$sslmode" \
		>$tmp_dir/${command_id}.sql
	if [ ! -s "$tmp_dir/${command_id}.sql" ]; then
		sleep 20
		run_psql_ssl "$command" "$uri" "postgres" "" "$sslmode" \
			>$tmp_dir/${command_id}.sql
	fi
	diff -u $expected_result $tmp_dir/${command_id}.sql
}

run_psql_ssl() {
	local command="$1"
	local uri="$2"
	local driver=${3:-postgres}

	local client_container=$(kubectl_bin get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')
	local sslmode="$5"

	kubectl_bin exec ${client_container} -- \
		bash -c "printf '$command\n' | PGSSLMODE=$sslmode PGSSLROOTCERT=/tmp/tls/ca.crt psql -v ON_ERROR_STOP=1 -t -q $driver://$uri" || true
}

patch_secret() {
	local secret=$1
	local key=$2
	local value=$3

	kubectl_bin patch secret $secret -p="{\"data\":{\"$key\": \"$value\"}}"
}
