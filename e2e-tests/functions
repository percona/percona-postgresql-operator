#!/bin/bash

# set root repo relatively to a test dir
ROOT_REPO=${ROOT_REPO:-$(realpath ../../..)}
source "${ROOT_REPO}/e2e-tests/vars.sh"
test_name=$(basename "$(pwd)")

create_namespace() {
	local namespace=$1
	if [[ ${CLEAN_NAMESPACE} == 1 ]]; then
		kubectl delete namespace $namespace --ignore-not-found || :
		kubectl wait --for=delete namespace "$namespace" || :
	fi

	kubectl create namespace $namespace
}

deploy_operator() {
	if [ -n "$OPERATOR_NS" ]; then
		create_namespace $OPERATOR_NS
		kubectl -n "${OPERATOR_NS}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/crd.yaml"
		kubectl -n "${OPERATOR_NS}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/cw-rbac.yaml"
		yq eval \
			"$(printf 'select(documentIndex==0).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
			"${DEPLOY_DIR}/cw-operator.yaml" \
			| kubectl -n "${OPERATOR_NS}" apply -f -
	else
		kubectl -n "${NAMESPACE}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/crd.yaml"
		kubectl -n "${NAMESPACE}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/rbac.yaml"
		yq eval \
			"$(printf 'select(documentIndex==0).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
			"${DEPLOY_DIR}/operator.yaml" \
			| kubectl -n "${NAMESPACE}" apply -f -
	fi
}

deploy_s3_secrets() {
	set +o xtrace
	printf "[global]\nrepo1-s3-key=%s\nrepo1-s3-key-secret=%s\n" \
		"$(yq eval 'select(.metadata.name=="*s3*").data.AWS_ACCESS_KEY_ID' "${TESTS_CONFIG_DIR}/cloud-secret.yml" | base64 -d)" \
		"$(yq eval 'select(.metadata.name=="*s3*").data.AWS_SECRET_ACCESS_KEY' "${TESTS_CONFIG_DIR}/cloud-secret.yml" | base64 -d)" \
		>"${TEMP_DIR}/pgbackrest-secret.ini"

	case ${test_name} in
		scheduled-backup)
			printf 'repo2-gcs-key=/etc/pgbackrest/conf.d/gcs-key.json\n' >>"${TEMP_DIR}/pgbackrest-secret.ini"
			yq eval '.stringData["credentials.json"]' ${TESTS_CONFIG_DIR}/cloud-secret-minio-gw.yml >${TEMP_DIR}/gcs-key.json
			kubectl -n "${NAMESPACE}" create secret generic "${test_name}-pgbackrest-secrets" --from-file=cloud.conf="${TEMP_DIR}/pgbackrest-secret.ini" --from-file=gcs-key.json=${TEMP_DIR}/gcs-key.json
			;;
		*)
			kubectl -n "${NAMESPACE}" create secret generic "${test_name}-pgbackrest-secrets" --from-file=cloud.conf="${TEMP_DIR}/pgbackrest-secret.ini"
			;;
	esac

	set -o xtrace
}

deploy_client() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/client.yaml"
}

get_cr() {
	local name_suffix=$1

	yq eval '
		'$(printf .metadata.name="%s" \"${test_name}\")'
		| .metadata.labels={"e2e":"'${test_name}'"}
		| .spec.postgresVersion='${PG_VER}'
		| .spec.users += [{"name":"postgres","password":{"type":"AlphaNumeric"}}]
		| .spec.users += [{"name":"'${test_name}'","password":{"type":"AlphaNumeric"}}]
		| '$(printf .spec.image="%s" \"${IMAGE_POSTGRESQL}\")'
		| '$(printf .spec.backups.pgbackrest.image="%s" \"${IMAGE_BACKREST}\")'
		| '$(printf .spec.proxy.pgBouncer.image="%s" \"${IMAGE_PGBOUNCER}\")'
		| '$(printf .spec.pmm.image="%s" \"${IMAGE_PMM}\")'
		| '$(printf .spec.pmm.secret="%s" \"${test_name}-pmm-secret\")'
		' "${DEPLOY_DIR}/cr.yaml" \
		>"${TEMP_DIR}/cr.yaml"

	case ${test_name} in
		"demand-backup" | "start-from-backup")
			yq eval '
				.spec.backups.pgbackrest.configuration = [{"secret":{"name":"'${test_name}'-pgbackrest-secrets"}}]
				| .spec.backups.pgbackrest.manual.repoName = "repo1"
				| .spec.backups.pgbackrest.manual.options = ["--type=full"]
				| .spec.backups.pgbackrest.global.repo1-path = "/backrestrepo/postgres-operator/'${test_name}${name_suffix:+-$name_suffix}'/repo1"
				| .spec.backups.pgbackrest.repos = [{"name":"repo1","s3":{"bucket":"'${BUCKET}'","endpoint":"s3.amazonaws.com","region":"us-east-1"}}]
				' "${TEMP_DIR}/cr.yaml" \
				>"${TEMP_DIR}/backup.cr.yaml"
			mv "${TEMP_DIR}/backup.cr.yaml" "${TEMP_DIR}/cr.yaml"
			if [[ ${test_name} == "start-from-backup" ]]; then
				yq eval '
					.spec.dataSource.pgbackrest.configuration = [{"secret":{"name":"'${test_name}'-pgbackrest-secrets"}}]
					| .spec.dataSource.pgbackrest.stanza = "db"
					| .spec.dataSource.pgbackrest.global.repo1-path = "/cluster-source/demand-backup-ppg'${PG_VER}'/repo1"
					| .spec.dataSource.pgbackrest.repo = {"name":"repo1","s3":{"bucket":"'${BUCKET}'","endpoint":"s3.amazonaws.com","region":"us-east-1"}}
					' "${TEMP_DIR}/cr.yaml" \
					>"${TEMP_DIR}/start-from-backup.cr.yaml"
				mv "${TEMP_DIR}/start-from-backup.cr.yaml" "${TEMP_DIR}/cr.yaml"
			fi
			;;
		scheduled-backup)
			yq eval '
				.spec.backups.pgbackrest.configuration = [{"secret":{"name":"'${test_name}'-pgbackrest-secrets"}}]
				| .spec.backups.pgbackrest.manual.repoName = "repo1"
				| .spec.backups.pgbackrest.manual.options = ["--type=full"]
				| .spec.backups.pgbackrest.global.repo1-path = "/backrestrepo/postgres-operator/'${test_name}${name_suffix:+-$name_suffix}'/repo1"
				| .spec.backups.pgbackrest.global.repo2-path = "/backrestrepo/postgres-operator/'${test_name}${name_suffix:+-$name_suffix}'/repo2"
				| .spec.backups.pgbackrest.repos = [{"name":"repo1","s3":{"bucket":"'${BUCKET}'","endpoint":"s3.amazonaws.com","region":"us-east-1"}}]
				| .spec.backups.pgbackrest.repos += [{"name":"repo2","gcs":{"bucket":"'${BUCKET}'"}}]
				' "${TEMP_DIR}/cr.yaml" \
				>"${TEMP_DIR}/backup.cr.yaml"
			mv "${TEMP_DIR}/backup.cr.yaml" "${TEMP_DIR}/cr.yaml"
			;;
		*) ;;
	esac
	cat "${TEMP_DIR}/cr.yaml"
}

run_psql() {
	local command=${1}
	local uri=${2}
	local driver=${3:-postgres}
	local client_container=$(kubectl -n ${NAMESPACE} get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl -n ${NAMESPACE} exec ${client_container} -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://'$uri'"
}

get_psql_user_pass() {
	local secret_name=${1}

	kubectl -n ${NAMESPACE} get "secret/${secret_name}" --template='{{.data.password | base64decode}}'
}

get_psql_user_host() {
	local secret_name=${1}

	kubectl -n ${NAMESPACE} get "secret/${secret_name}" --template='{{.data.host | base64decode }}'
}

get_instance_set_pods() {
	local instance=${1:-instance1}

	kubectl get pods -n ${NAMESPACE} --selector postgres-operator.crunchydata.com/instance-set=${instance} -o custom-columns='NAME:.metadata.name' --no-headers
}

get_psql_pod_host() {
	local pod=${1}

	echo "${pod}.${test_name}-pods.${NAMESPACE}.svc"
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl get pod/$pod -n "${NAMESPACE}" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl describe pod/$pod -n "${NAMESPACE}"
			kubectl logs $pod -n "${NAMESPACE}"
			kubectl logs $(get_operator_pod) ${OPERATOR_NS:+-n $OPERATOR_NS} \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

get_service_ip() {
	local service=$1
	while (kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}' 2>&1 || :) | grep -q NotFound; do
		sleep 1
	done
	if [ "$(kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
		kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.clusterIP}'
		return
	fi
	until kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
		sleep 1
	done
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

wait_for_delete() {
	local res="$1"

	echo -n "$res - "
	retry=0
	until (kubectl get $res -n "${NAMESPACE}" || :) 2>&1 | grep NotFound; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 120 ]; then
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
}

deploy_pmm_server() {
	local platform=kubernetes
	helm uninstall -n "${NAMESPACE}" pmm || :
	helm install monitoring -n "${NAMESPACE}" --set imageTag=$IMAGE_PMM_SERVER_TAG --set service.type="LoadBalancer" \
		--set imageRepo=$IMAGE_PMM_SERVER_REPO --set platform="${platform}" "https://percona-charts.storage.googleapis.com/pmm-server-${PMM_SERVER_VERSION}.tgz"
}

generate_pmm_api_key() {
	local ADMIN_PASSWORD=$(kubectl -n "${NAMESPACE}" exec monitoring-0 -- bash -c "printenv | grep ADMIN_PASSWORD | cut -d '=' -f2")
	local PMM_SERVICE_IP=$(get_service_ip monitoring-service)
	curl \
		--insecure \
		-X POST \
		-H "Content-Type: application/json" \
		-d '{"name":"'${RANDOM}'", "role": "Admin"}' \
		--user "admin:${ADMIN_PASSWORD}" \
		"https://${PMM_SERVICE_IP}/graph/api/auth/keys" \
		| jq -r .key
}

get_metric_values() {
	local metric=$1
	local instance=$2
	local api_key=$3
	local start=$($date -u "+%s" -d "-1 minute")
	local end=$($date -u "+%s")
	local endpoint=$(get_service_ip monitoring-service)

	curl -s -k -H "Authorization: Bearer ${api_key}" "https://$endpoint/graph/api/datasources/proxy/1/api/v1/query_range?query=min%28$metric%7Bnode_name%3D%7E%22$instance%22%7d%20or%20$metric%7Bnode_name%3D%7E%22$instance%22%7D%29&start=$start&end=$end&step=60" \
		| jq '.data.result[0].values[][1]' \
		| grep '^"[0-9]'
}

get_qan20_values() {
	local instance=$1
	local api_key=$2
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=$(get_service_ip monitoring-service)

	cat >payload.json <<EOF
{
   "columns":[
	  "load",
	  "num_queries",
	  "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
	   {
		   "key": "cluster",
		   "value": ["postgresql"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF

	curl -s -k -H "Authorization: Bearer ${api_key}" -XPOST -d @payload.json "https://$endpoint/v0/qan/GetReport" \
		| jq '.rows[].sparkline'
	rm -f payload.json
}
