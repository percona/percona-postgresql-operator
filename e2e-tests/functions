#!/bin/bash

# set root repo relatively to a test dir
ROOT_REPO=${ROOT_REPO:-$(realpath ../../..)}
source "${ROOT_REPO}/e2e-tests/vars.sh"
test_name=$(basename "$(pwd)")

deploy_operator() {
	kubectl -n "${NAMESPACE}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/crd.yaml"
	kubectl -n "${NAMESPACE}" apply --server-side --force-conflicts -f "${DEPLOY_DIR}/rbac.yaml"

	yq eval \
		"$(printf 'select(documentIndex==0).spec.template.spec.containers[0].image="%s"' "${IMAGE}")" \
		"${DEPLOY_DIR}/operator.yaml" \
		| kubectl -n "${NAMESPACE}" apply -f -

}

deploy_s3_secrets() {
	set +o xtrace
	printf "[global]\nrepo1-s3-key=%s\nrepo1-s3-key-secret=%s\n" \
		"$(yq eval 'select(.metadata.name=="*s3*").data.AWS_ACCESS_KEY_ID' "${TESTS_CONFIG_DIR}/cloud-secret.yml" | base64 -d)" \
		"$(yq eval 'select(.metadata.name=="*s3*").data.AWS_SECRET_ACCESS_KEY' "${TESTS_CONFIG_DIR}/cloud-secret.yml" | base64 -d)" \
		>"${TEMP_DIR}/pgbackrest-secret.ini"
	set -o xtrace
	kubectl -n "${NAMESPACE}" create secret generic "${test_name}-pgbackrest-secrets" --from-file=s3.conf="${TEMP_DIR}/pgbackrest-secret.ini"
}

deploy_client() {
	kubectl -n "${NAMESPACE}" apply -f "${TESTS_CONFIG_DIR}/client.yaml"
}

get_cr() {
	local name_suffix=$1

	yq eval "$(printf '.metadata.name="%s"' "${test_name}")" "${DEPLOY_DIR}/cr.yaml" \
		| yq eval '.spec.postgresVersion='${PG_VER}'' - \
		| yq eval '.spec.users += [{"name":"postgres","password":{"type":"AlphaNumeric"}}]' - \
		| yq eval '.spec.users += [{"name":"'${test_name}'","password":{"type":"AlphaNumeric"}}]' - \
		| yq eval "$(printf '.spec.image="%s"' "${IMAGE_POSTGRESQL}")" - \
		| yq eval "$(printf '.spec.backups.pgbackrest.image="%s"' "${IMAGE_BACKREST}")" - \
		| yq eval "$(printf '.spec.proxy.pgBouncer.image="%s"' "${IMAGE_PGBOUNCER}")" - \
		| yq eval "$(printf '.spec.pmm.secret="%s"' "${test_name}-pmm-secret")" - \
			>"${TEMP_DIR}/cr.yaml"

	case ${test_name} in
		"demand-backup" | "start-from-backup")
			yq eval '.spec.backups.pgbackrest.configuration = [{"secret":{"name":"'${test_name}'-pgbackrest-secrets"}}]' "${TEMP_DIR}/cr.yaml" \
				| yq eval '.spec.backups.pgbackrest.manual.repoName = "repo1"' - \
				| yq eval '.spec.backups.pgbackrest.manual.options = ["--type=full"]' - \
				| yq eval '.spec.backups.pgbackrest.global.repo1-path = "/backrestrepo/postgres-operator/'${test_name}${name_suffix:+-$name_suffix}'/repo1"' - \
				| yq eval '.spec.backups.pgbackrest.repos = [{"name":"repo1","s3":{"bucket":"'${BUCKET}'","endpoint":"s3.amazonaws.com","region":"us-east-1"}}]' - \
					>"${TEMP_DIR}/backup.cr.yaml"
			mv "${TEMP_DIR}/backup.cr.yaml" "${TEMP_DIR}/cr.yaml"
			if [[ ${test_name} == "start-from-backup" ]]; then
				yq eval '.spec.dataSource.pgbackrest.configuration = [{"secret":{"name":"'${test_name}'-pgbackrest-secrets"}}]' "${TEMP_DIR}/cr.yaml" \
					| yq eval '.spec.dataSource.pgbackrest.stanza = "db"' - \
					| yq eval '.spec.dataSource.pgbackrest.global.repo1-path = "/backrestrepo/postgres-operator/demand-backup-ppg'${PG_VER}'/repo1"' - \
					| yq eval '.spec.dataSource.pgbackrest.repo = {"name":"repo1","s3":{"bucket":"'${BUCKET}'","endpoint":"s3.amazonaws.com","region":"us-east-1"}}' - \
						>"${TEMP_DIR}/start-from-backup.cr.yaml"
				mv "${TEMP_DIR}/start-from-backup.cr.yaml" "${TEMP_DIR}/cr.yaml"
			fi
			;;
		*) ;;
	esac
	cat "${TEMP_DIR}/cr.yaml"
}

run_psql() {
	local command=${1}
	local uri=${2}
	local driver=${3:-postgres}
	local client_container=$(kubectl -n ${NAMESPACE} get pods --selector=name=pg-client -o 'jsonpath={.items[].metadata.name}')

	kubectl -n ${NAMESPACE} exec ${client_container} -- \
		bash -c "printf '$command\n' | psql -v ON_ERROR_STOP=1 -t -q $driver://'$uri'"
}

get_psql_user_pass() {
	local user=${1}
	local cluster=${2:-${test_name}}

	kubectl -n ${NAMESPACE} get "secret/${cluster}-pguser-${user}" -o jsonpath='{.data.password}' | base64 -d
}

get_psql_user_host() {
	local user=${1}
	local cluster=${2:-${test_name}}

	kubectl -n ${NAMESPACE} get "secret/${cluster}-pguser-${user}" -o jsonpath='{.data.host}' | base64 -d
}

wait_pod() {
	local pod=$1

	set +o xtrace
	retry=0
	echo -n $pod
	until kubectl get pod/$pod -n "${NAMESPACE}" -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
		sleep 1
		echo -n .
		let retry+=1
		if [ $retry -ge 360 ]; then
			kubectl describe pod/$pod -n "${NAMESPACE}"
			kubectl logs $pod -n "${NAMESPACE}"
			kubectl logs $(get_operator_pod) -n "${NAMESPACE}" \
				| grep -v 'level=info' \
				| grep -v 'level=debug' \
				| grep -v 'Getting tasks for pod' \
				| grep -v 'Getting pods from source' \
				| tail -100
			echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
			exit 1
		fi
	done
	set -o xtrace
}

get_service_ip() {
	local service=$1
	while (kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}' 2>&1 || :) | grep -q NotFound; do
		sleep 1
	done
	if [ "$(kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
		kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.spec.clusterIP}'
		return
	fi
	until kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
		sleep 1
	done
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
	kubectl get service/$service -n "${NAMESPACE}" -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
}

deploy_pmm_server() {
	helm uninstall -n "${NAMESPACE}" pmm || :
	kubectl delete clusterrolebindings.rbac.authorization.k8s.io pmm || :
	kubectl delete clusterrole.rbac.authorization.k8s.io pmm || :
	helm repo remove percona || :
	helm repo add percona https://percona.github.io/percona-helm-charts/
	helm install -n "${NAMESPACE}" pmm percona/pmm --set service.type="LoadBalancer"
	wait_pod pmm-0
}

generate_pmm_api_key() {
	local admin_pass=$(kubectl -n "${NAMESPACE}" get secret pmm-secret -o yaml | yq eval '.data.PMM_ADMIN_PASSWORD' - | base64 -d)
	local pmm_ip=$(get_service_ip monitoring-service)
	curl \
		--insecure \
		-X POST \
		-H "Content-Type: application/json" \
		-d '{"name":"operator", "role": "Admin"}' \
		--user "admin:${admin_pass}" \
		"https://${pmm_ip}/graph/api/auth/keys" \
		| jq -r .key
}

function get_metric_values() {
	local metric=$1
	local instance=$2
	local api_key=$3
	local start=$($date -u "+%s" -d "-1 minute")
	local end=$($date -u "+%s")
	local endpoint=$(get_service_ip monitoring-service)

	curl -s -k -H "Authorization: Bearer ${api_key}" "https://$endpoint/graph/api/datasources/proxy/1/api/v1/query_range?query=min%28$metric%7Bnode_name%3D%7E%22$instance%22%7d%20or%20$metric%7Bnode_name%3D%7E%22$instance%22%7D%29&start=$start&end=$end&step=60" \
		| jq '.data.result[0].values[][1]' \
		| grep '^"[0-9]'
}

function get_qan20_values() {
	local instance=$1
	local api_key=$2
	local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
	local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
	local endpoint=$(get_service_ip monitoring-service)

	cat >payload.json <<EOF
{
   "columns":[
	  "load",
	  "num_queries",
	  "query_time"
   ],
   "first_seen": false,
   "group_by": "queryid",
   "include_only_fields": [],
   "keyword": "",
   "labels": [
	   {
		   "key": "cluster",
		   "value": ["postgresql"]
   }],
   "limit": 10,
   "offset": 0,
   "order_by": "-load",
   "main_metric": "load",
   "period_start_from": "$($date -u -d '-12 hour' '+%Y-%m-%dT%H:%M:%S%:z')",
   "period_start_to": "$($date -u '+%Y-%m-%dT%H:%M:%S%:z')"
}
EOF

	curl -s -k -H "Authorization: Bearer ${api_key}" -XPOST -d @payload.json "https://$endpoint/v0/qan/GetReport" \
		| jq '.rows[].sparkline'
	rm -f payload.json
}